{{template "base" .}}

{{define "title"}}XVI wiek - zapiski{{end}}
{{define "meta_description"}}"Zapiski (podstrona serwisu XVI wiek - ciekawe wydarzenia historyczne w Polsce i krajach sąsiednich)"{{end}}
{{define "meta_title"}}"XVI wiek - zapiski"{{end}}
{{define "canonical"}}<link rel="canonical" href="https://xvi-wiek.pl/zapiski" />{{end}}

{{define "main"}}

    <h1 id="tufte-css">Zapiski</h1>

    <div class="preface">Zapiski, notatki, artykuły i ciekawostki dotyczące programowania, historii, archeologii,
    humanistyki cyfrowej...
    </div>
    <p></p>

    <div>
    <p>
    Spis treści:<br><br>
    <strong>2022-09-25</strong> <a href="#2022-09-25">OpenRefine - własny serwis rekoncyliacji/uspójniania</a><br>
    <strong>2022-10-01</strong> <a href="#2022-10-01">spaCy, NER i półautomatyczne tworzenie indeksu postaci</a><br>
    <strong>2022-10-03</strong> <a href="#2022-10-03">spaCy, POS i Matcher - czyli do czego przydają się rzeczowniki</a><br>
    <strong>2022-10-05</strong> <a href="#2022-10-05">spaCy - jak poprawić wyniki NER czyli Barnim XI to też osoba...</a><br>
    <strong>2022-10-06</strong> <a href="#2022-10-06">Artykuły, kursy, filmy - o NLP w badaniach historycznych<a/><br>
    <strong>2022-10-07</strong> <a href="#2022-10-07">Anotacja tekstów w Doccano i trenowanie własnego modelu<a/><br>
    </p>
    </div>

    <article>
        <hr>
        <h2 id="2022-09-25">OpenRefine - własny serwis rekoncyliacji/uspójniania (<strong>25.09.2022</strong>)</h2>

        <section>
            <p>
                <a href="https://openrefine.org/" target="_blank" rel="noopener">OpenRefine</a>
                jest popularnym narzędziem do oczyszczania i przekształcania danych.

                <label for="%s" class="margin-toggle sidenote-number"></label>
                <input type="checkbox" id="%s" class="margin-toggle"/>
                <span class="sidenote">Polecam artykuły na temat OpenRefine:
                <a href="https://www.pilsudski.org/pl/instytut-cyfrowy/pracownia-digitalizacji/blog/869-oczyszczanie-danych-z-uzyciem-openrefine" target="_blank" rel="noopener">
                Oczyszczanie danych z użyciem OpenRefine
                </a>
                oraz
                <a href="http://programminghistorian.org/en/lessons/cleaning-data-with-openrefine" target="_blank" rel="noopener">
                Cleaning Data with OpenRefine
                </a>
                </span>

                Jedną z jego ciekawszych funkcji jest możliwość rekoncyliacji danych
                przy wykorzystaniu zewnętrznych serwisów udostępniających dane do uspójniania. Często zdarzają 
                się w danych historycznych różnorakie sposoby zapisu imion i nazwisk postaci, lub nazw geograficznych.
                Nazwy miejscowości mogą występować w brzmieniu współczesnym, w formie używanej w XVI wieku,
                lub po prostu w formie błędnie zapisanej w źródle. Mechanizm uspójniania w OpenRefine pozwala
                uzgodnić nazwę występującą w naszych danych z nazwą pochodzącą z pewnego źródła. Np. powiązać
                "Zbigniewa Oleśnickiego" z identyfikatorem z bazy VIAF, lub powiązać miejscowość Brzozowa (występująca
                w źródłach także jako Brosoua) z identyfikatorem z Atlasu Historycznego Polski (Brzozowa_sdc_krk).
            </p>
            <p> Problemem może być znalezienie odpowiedniego serwisu rekoncyliacji. O ile popularne źródła
                w rodzaju VIAF czy wikidata.org są dostępne

                <label for="%s" class="margin-toggle sidenote-number"></label>
                <input type="checkbox" id="%s" class="margin-toggle"/>
                <span class="sidenote"><a href="https://github.com/OpenRefine/OpenRefine/wiki/Reconcilable-Data-Sources" target="_blank" rel="noopener">Lista</a>
                dostępnych serwerów rekoncyliacji
                </span>

                , o tyle uspójnianie z bazą miejscowości AHP lub naszą lokalną
                bazą osób może wymagać uruchomienia własnego serwisu. Nie jest to jednak takie trudne, istnieją
                bowiem gotowe narzędzia, które można wykorzystać do tego celu. Jednym z nich jest
                <strong>Reconcile-csv</strong> udostępniony (na otwartej licencji BSD-2) na stronie
                <a href="http://okfnlabs.org/reconcile-csv/" target="_blank" rel="noopener">Open Knowledge Lab</a>.
                Program ten pozwala na dopasowanie szukanej przez nas nazwy do nazw w swojej bazie poprzez
                mechanizm przybliżonego (rozmytego) porównywania (fuzzy matching), nazwy nie muszą
                być identyczne, mogą być podobne, program będzie wówczas proponował listę zbliżonych
                do podanej nazw wraz ze współczynnikiem podobieństwa. Bazą dla Reconcile-csv
                jest plik tekstowy w formacie CSV.  
            </p>

            <p>
            Program jest plikiem *.jar,
            (warto pobrać z <a href="https://github.com/okfn/reconcile-csv" target="_blank" rel="noopener">githuba</a>
            też plik index.html.tpl, który jest szablonem
            wyświetlanym jako główna strona serwisu) do uruchomienia potrzebuje więc zainstalowanej Javy (JRE),
            potrzebny jest oczywiście plik CSV z danymi (plik w którym każdy wiersz jest rekordem
            danych, pola rozdzielone są przecinkami a pierwszy wiersz zawiera nazwy kolumn).
            </p>

            <p>
            Polecenie uruchamiające serwer:<br>
            <code>
                java -Xmx2g -jar reconcile-csv-0.1.2.jar plik.csv search_column id_column
            </code>
            </p>

            <p>
            gdzie <code>plik.csv</code> to nazwa naszego pliku z danymi, <code>search_columm</code>
            to nazwa pola w pliku z danymi, które będzie służyło do rekoncyliacji w OpenRefine,
            <code>id_column</code> to nazwa pola w pliku z danymi, które zawiera jednoznaczny identyfikator danych.
            </p>

            <p>Po uruchomieniu serwis dostępny jest pod adresem <em>http://localhost:8000/reconcile</em>,
            i taki adres należy wprowadzić w OpenRefine (zwraca on zawartość w formacie json, czytelną dla
            programu - odbiorcy danych z serwisu). Adres główny serwisu (<em>http://localhost:8000</em>)
            wyświetli o nim podstawowe informacje w formie czytelnej dla człowieka.
            </p>
            <img src="/static/img_zapiski/reconcile_csv_01.png" width="500" alt="serwis reconcile-csv"/>

            <p>
            Lokalny serwis rekoncyliacji podłącza się do OpenRefine podobnie jak serwisy zewnętrzne,
            po uruchomieniu rekoncyliacji na wybranej kolumnie danych przycisk Add Standard Service wyświetli
            okienko w którym można podać adres serwisu, zaakceptowany serwis pojawi się na liście.
            <label for="%s" class="margin-toggle sidenote-number"></label>
            <input type="checkbox" id="%s" class="margin-toggle"/>
            <span class="sidenote">Standardowa nazwa serwisu będzie różnić się od pokazanej na
            ilustracji i wygląda tak: 'CSV Reconciliation service'. Jest niestety zaszyta w kodzie źródłowym
            Reconcile-csv i jej zmiana wymaga rekompilacji programu (zob. dalszą część artykułu).
            </span>
            </p>
            <img src="/static/img_zapiski/reconcile_csv_04.png" width="500" alt="add standard service"/>

            <p>
            Można wówczas rozpocząć proces uspójniania. Dane które według mechanizmu zostały pewnie dopasowane
            (współczynnik > 0.85) zostaną od razu przypisane do komórek kolumny dla której przeprowadzamy
            rekoncyliację, w innych przypadkach wyświetlona zostanie lista wraz z wartościami dopasowania.
            Dla każdej z pozycji można wyświetlić informacje z pliku csv będącego źródłem danych serwisu, na
            przykładzie poniżej są to dane miejscowości z Atlasu Historycznego Polski.
            <label for="%s" class="margin-toggle sidenote-number"></label>
            <input type="checkbox" id="%s" class="margin-toggle"/>
            <span class="sidenote">Dostępne publicznie na licencji CC BY-ND 4.0:
            <a href="https://data.atlasfontium.pl/documents/202?_ga=2.122616834.865899583.1640103019-2000553280.1624037407" target="_blank" rel="noopener">
            AHP 2.0</a> (IH PAN).
            </span>
            </p>
            <img src="/static/img_zapiski/reconcile_csv_11.png" width="500" alt="reconcile_view"/>

            <p><strong>Jak zmodyfikować program reconcile-csv</strong>:
            zarówno zmiana nazwy serwisu (nazwy wyświetlanej w OpenRefine), jak i dostosowanie programu
            Reconcile-csv do pracy na serwerze, wymaga zmian w jego kodzie źródłowym
            <label for="%s" class="margin-toggle sidenote-number"></label>
            <input type="checkbox" id="%s" class="margin-toggle"/>
            <span class="sidenote">Repozytorium aplikacji w serwisie
            <a href="https://github.com/okfn/reconcile-csv">github</a>.
            </span>
            a w związku z tym pewnych umiejętności 'programistycznych'.
            Sama zmiana nazwy serwisu wymaga jednak tylko modyfikacji 50 wiersza pliku
            /src/reconcile_csv/core.clj: <code> {:name "CSV Reconciliation service"</code>,
            zmiany tekstu <em>"CSV Reconciliation service"</em> na nowy i rekompilacji programu.
            Aplikacja została stworzona w języku clojure. Najłatwiejszym sposobem zarządzania i kompilacji projektu
            w języku clojure jest użycie systemu
            <a href="https://leiningen.org/" target="_blank" rel="noopener">Leiningen</a>, który w systemie
            Ubuntu można zainstalować poleceniem: <code>sudo apt install leiningen</code>.
            Rekompilacja projektu Reconcile-csv i budowa nowego pliku *.jar sprowadza się do
            uruchomienia polecenia: <code>lein uberjar</code> w katalogu z plikiem project.cli,
            skompilowany plik *.jar powinien znajdować się w podkatalogu <code>/target</code>.
            </p>

            <p>
            Modyfikacja kodu i rekompilacja za każdym razem kiedy chcemy zmienić np. nazwę serwisu
            rekoncyliacji wyświetlaną w OpenRefine może być jednak dość niewygodna. Dlatego przygotowałem małą
            modyfikację programu reconcile-cvs która obsługuje cztery dodatkowe parametry uruchamiania z linii komend:
            <code>adres_serwera</code>, <code>port</code>, <code>nazwa_serwisu_rekoncyliacji</code> i <code>nazwa_typu</code>.
            Dodatkowo serwis wyświetla trochę bardziej czytelny podgląd danych z pliku csv, podczas
            uspójniania (widoczne ramki tabeli).
            <img src="/static/img_zapiski/reconcile_csv_table.jpg" width="500" alt="reconcile_view"/>
            </p>

            <p>Przykładowe wywołanie:<br>
            <code>
             java -Xmx2g -jar reconcile-csv-0.1.2.jar plik.csv search_column id_column "http://moj_serwer.org" "80" "Miasta-Osoby Serwis Rekoncyliacji" "miasta_osoby"
            </code>
            <p>
            <p>
            lub (obecnie port inny niż 80 trzeba podać także w adresie serwera):<br>
            <code>
             java -Xmx2g -jar reconcile-csv-0.1.2.jar plik.csv search_column id_column "http://localhost:8000" "8000" "Miasta-Osoby Serwis Rekoncyliacji" "miasta_osoby"
            </code>
            </p>

            <p>Kod źródłowy modyfikacji można pobrać z <a href="https://github.com/pjaskulski/reconcile-csv" target="_blank" rel="noopener">github</a>,
            skompilowany plik jar również: <a href="https://github.com/pjaskulski/reconcile-csv/releases" target="_blank" rel="noopener">releases</a>.
            </p>
        </section>
    </article>

    <article>
        <hr>
        <h2 id="2022-10-01">spaCy, NER i półautomatyczne tworzenie indeksu postaci (<strong>01.10.2022</strong>)</h2>

        <section>

        <p>Postęp rozwoju metod przetwarzania języka naturalnego (NLP) w ciągu ostatnich kilku lat pozwala
        na coraz szersze ich użycie, także w odniesieniu do tekstów w języku polskim. Jedną z technik NLP
        jest rozpoznawanie jednostek nazwanych (NER - named entity recogition), w czystym, pozbawionym struktury
        tekście poddanym analizie NER można oznaczyć np. osoby, miejsca czy daty. Do czego może się to przydać w przypadku
        publikacji historycznych? Jednym z najprostszych przypadków użycia jaki przychodzi do głowy jest stworzenie
        indeksu osób dla publikacji gdzie takiego indeksu brakuje.</p>

        <p>Do testu posłuży artykuł (właściwie książka, sądząc po objętości) Stanisława Bodniaka, "Polska a Bałtyk za ostatniego z Jagiellona"
        wydana w 1946 roku (<em>Pamiętnik Biblioteki Kórnickiej</em> 3, 42-276, 1939-46).
        <label for="%s" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="%s" class="margin-toggle"/>
        <span class="sidenote">
        Dzieło to ma zresztą ciekawą historię gdyż pierwszy druk i rękopis zostały utracone
        w wyniku działań wojennych, autor z zachowanych korekt i notatek
        odtworzył i wydał je po wojnie.
        </span>
        Publikacja została zdigitalizowana przez Muzeum Historii Polski i udostępniona w bazie
        bazhum.muzhp.pl w formie pdf.
        </p>
        <p>
        Sytuacja jednak nie jest idealna - pdf nie ma indeksu (nie wiem czy jest w papierowym
        wydaniu), ma warstwę OCR więc jest przeszukiwalny, jednak jakość OCR nie jest perfekcyjna,
        w tekście pojawiają się np. przypadkowe spacje wewnątrz słów. Aby to poprawić można
        proces ocr przeprowadzić powtórnie.
        <label for="%s" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="%s" class="margin-toggle"/>
        <span class="sidenote">Ponieważ na codzień pracuję w systemie Linux, narzędzia których używam związane są z
        tym systemem, jednak wiele z nich pracuje także pod Windows/Mac lub istnieją odpowiedniki
        dla tych środowisk.
        </span>
        </p>
        <img src="/static/img_zapiski/tekst_z_warstwy_ocr_pdfa.png" width="500" alt="reconcile_view"/>

        <p>Ciekawym narzędziem do wprowadzania warstwy tekstowej ocr do plików pdf jest
        <a href="https://ocrmypdf.readthedocs.io/en/latest/introduction.html">ocrmypdf</a>, może
        on rówież nadpisać istniejącą warstwę tekstową czy zapisać rozpoznany przez ocr na nowo
        tekst w osobnym pliku tekstowym (warstwa tekstowa zapisana przez ten program w pdf
        również nie była rewelacyjna, ale sam plik z tekstem okazał się niezły i nadawał się
        do dalszej obróbki). Silnikiem OCR używanym przez omawianą aplikację jest Tesseract,
        uważany za najlepszy mechanizm OCR open source.
        </p>
        <p>
        Polecenie uruchamiające przetwarzanie pdf-a wygląda tak:<br>
        <code>
        ocrmypdf -l pol --force-ocr --sidecar output.txt Bodniak_input.pdf Bodniak_output.pdf
        </code>
        </p>

        <p>
        gdzie opcja: <code>-l pol</code> odpowiada za obsługę języka polskiego,
        <code>--force-ocr</code> wymusza nadpisanie istniejącej warstwy ocr w pdf,
        zaś opcja <code>--sidecar output.txt</code> powoduje zapisanie tekstu z pdf
        w pliku tekstowym. Nazwy plików pdf na końcu to odpowiednio oryginalny plik
        oraz wyjściowy zmodyfikowany plik pdf.
        </p>

        <p>
        Oryginalny plik pdf pobrany z repozytorium bazhum.muzhp.pl może wymagać wstępnego
        przetworzenia (program ocrmypdf zgłasza błąd "EncryptedPdfError: Input PDF is encrypted.
        The encryption must be removed to perform OCR"), służy do tego program qpdf uruchamiany
        z linii komend: <code>qpdf --decrypt plik_wejściowy.pdf plik wyjściowy.pdf</code>.
        </p>

        <p>
        Innym narzędziem, które może posłużyć do wydobycia tekstu z pliku pdf jest program
        gImageReader będący graficzną nakładką na silnik OCR Tesseract, oprócz samego procesu
        OCR ma on dodatkowe funkcje pozwalające usunąć znaki końca wiersza ze środka akapitów
        oraz połączyć wyrazy podzielone między wiersze (komercyjne programy OCR mają z pewnością
        dużo większe możliwości w zakresie oczyszczania i poprawiania tekstu po procesie OCR).
        </p>
        <img src="/static/img_zapiski/tekst_po_ocr_gImageReader_Tesseract.png" width="500" alt="tekst_gimage_reader"/>

        <p>Mając plik z tekstem (z zachowanymi numerami stron, które zamieniłem na tagi w postaci
        <code>[PAGE: 45]</code> na początku każdej strony) możemy poddać go próbnej analizie NER.
        Istnieje wiele narzędzi i modeli, które można wykorzystać do tego celu, dostępna
        jest np. kolekcja narzędzi projektu CLARIN-PL (analiza NER do przetestowania także:
        <a href="https://ws.clarin-pl.eu/ner">on-line</a> z wykorzystaniem narzędzia Liner2). W tym teście skorzystam jednak z
        popularnej biblioteki NLP: <a href="https://spacy.io/">spaCy</a> i skryptów w języku Python.
        </p>

        <p>Najwygodniejszym sposobem testowania różnych rozwiązań w Pythonie jest środowisko
        <a href="https://jupyter.org/">Jupyter Notebook</a>
        Sama instalacja i konfiguracja Pythona, Jupytera i spaCy mogłaby być tematem osobnej zapiski, ale
        procudury ich instalacji są przystępnie opisane na podanych wyżej stronach.
        </p>

        <p>
        Po uruchomieniu środowiska Jupyter i utworzeniu nowego notatnika należy zaimportować
        bibliotekę spaCy oraz dodatkowo mechanizm wizualizacji displacy. Kolejnym krokiem będzie
        załadowanie tekstu do analizy oraz modelu dla języka polskiego. Standardowo spaCy oferuje
        3 modele (zob. <a href="https://spacy.io/models/pl">https://spacy.io/models/pl</a>), mały,
        średni i duży, różniące się wielkością (od 20 do 500 MB) i dokładnością. Do celów tego testu
        wystarczy model średni 'pl_core_news_md' (<em>Polish pipeline optimized for CPU. Components:
        tok2vec, morphologizer, parser, lemmatizer (trainable_lemmatizer), tagger, senter, ner.</em>).
        </p>

        <p>
        <code>
        import spacy<br>
        from spacy import displacy<br><br>

        nlp = spacy.load('pl_core_news_md')<br>
        with open('../data/bodniak_baltyk_small.txt', 'r', encoding='utf-8') as f:<br>
            text = f.read()<br><br>

        doc = nlp(text)<br>
        displacy.render(doc, style='ent')<br>
        </code>
        </p>

        <p>
        Kod podany powyżej przeprowadza analizę NER wczytanego tekstu (jest to fragment
        artykułu S. Bodniaka, str. 43) oraz wyświetla wynik w bardzo przystępnej wizualnie formie,
        gdzie znalezione jednostki nazwane (osoby, daty i miejsca) oznaczone są kolorami
        i etykietą ('persName', 'date', 'placeName'). Jak widać mechanizm radzi sobie całkiem nieźle z typowymi przypadkami
        osób, znajduje określenia temporalne i miejscowości.
        </p>
        <img src="/static/img_zapiski/jupyter_spacy.png" width="685" alt="jupyter_spacy"/>

        <p>Niektóre z postaci, np. Barnim XI (książę pomorski, w dolnej części tekstu na
        załączonym zrzucie ekranu) pozostają jednak nierozpoznane, jako osoby rozpoznawane są natomiast
        błędnie wyrażenia typu: 'Odstraszała' czy 'Podniosłyby', model użyty do testu był
        przygotowany na podstawie bardziej współczesnych tekstów stąd imion i innych określeń
        używanych historyczne po prostu może nie znać. Analizowany tekst dotyczy XVI wieku, ale gdyby
        występowały w nim postacie średniowieczne typu 'Jan z Goźlic' jako osoba zostałoby
        uznane prawdopodobnie tylko imię Jan a nie całe wyrażenie. Zdarzają się też błędne
        rozpoznania miejsc jako osób itp. Czy można model 'przekonać' do bardziej poprawnego rozpoznawania
        takich określeń? Oczywiście, istnieją także zapewne lepsze modele, ale jest to temat na
        inny wpis.</p>

        <p>
        Mając tekst z informacjami o numerach stronach, możliwość rozpoznania postaci analizą NER,
        możemy przygotować ich indeks, posłuży do tego skrypt pythonowy create_index.py
        <label for="%s" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="%s" class="margin-toggle"/>
        <span class="sidenote">Do pobrania z serwisu <a href="https://github.com/pjaskulski/zapiski">github</a>.
        </span>
        w którym najpierw wczytywana jest treść każdej strony,
        wykonywana analiza NER, wyniki dopisywanie do słownika wyników, na koniec wyświetlana
        jest posortowana lista znalezionych postaci (266 osób, z tym że niektóre występują czasem
        podwójnie lub potrójnie z powodu np. literówek czy błędnie ustalonej formy podstawowej
        imienia i nazwiska) wraz z numerami stron. Lista nie jest z pewnością dokładna, wymaga
        oczyszczenia przez człowieka (występują wspomniane już wyżej błędne rozpoznania, problemy
        z odmianą imion i nazwisk, jakość ocr też wpływa na błędy wynikające z literówek) jednak
        z pewnością jest dużym ułatwieniem przy tworzeniu indeksu, no i jej przygotowaniem zajął
        się komputer, dając człowiekowi czas na zadania, których komputer na dziś nie jest w stanie
        się podjąć. Poniżej fragment przygotowanego indeksu po wstępnym oczyszczeniu.
        </p>

        <p>
        <code>
        Achacy Czema [262]<br>
        Adam Konarski [190, 273]<br>
        Adolf (holsztyński) [157]<br>
        Albrecht Giese [88, 120, 171]<br>
        Albrecht Wilkowski [116]<br>
        Albrecht (Hohenzollern) [43, 44, 51, 52, 56, 57, 58, 70, 134]<br>
        Aleksander Połubieński [216]<br>
        Andrzej Swarożyński [74, 81, 84, 104, 274]<br>
        Antoni Angela [44]<br>
        Bajerski [93, 94]<br>
        Barnim XI [126, 187]<br>
        Bąkowski [138]<br>
        Birsen [46]<br>
        Bogusław XIII [187]<br>
        Boleman [177, 178]<br>
        Chodkiewicz [101, 191, 207, 228, 263, 270, 271]<br>
        [...]<br>
        Łukasz Górnicki [43]<br>
        Łukasz Podoski [190, 222, 273]<br>
        Maciej Scharping [58, 60, 67, 74, 83]<br>
        Maciej Strubicz [77, 271]<br>
        </code>
        </p>

        </section>
    </article>

    <article>
        <hr>
        <h2 id="2022-10-03">spaCy, POS i Matcher - czyli do czego przydają się rzeczowniki
        (<strong>03.10.2022</strong>)</h2>

        <section>
        <p>Mając indeks postaci występujących w publikacji
        <label for="%s" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="%s" class="margin-toggle"/>
        <span class="sidenote">Zob. poprzednią <a href="#2022-10-01">zapiskę</a>.
        </span>
        wiemy kto występuje w tekście, ale
        czy wiemy kim był? W przypadku osób powszechnie znanych kojarzymy, że Zygmunt August
        był królem, Albrecht Hohenzollern władcą Prus, ale jakie funkcje pełnili, jakich zawodów
        byli przedstawicielami ci pozostali, mniej znani? Czy analizy NLP mogą  nam pomóc w
        odpowiedzi na te pytania? Tak i to nawet najbardziej podstawowe metody, jak określenia
        części mowy (POS - part of speech) oraz Matcher - silnik dopasowywania wzorców.
        </p>

        <p>
        Podczas przetwarzania tekstu spaCy dzieli go na zdania oraz tokeny, każdy token jest
        zwykle słowem lub znakiem interpunkcyjnym. Dla każdego z tokenów spacy określa jaką
        jest częścią mowy, można więc dzięki temu odseparować same rzeczowniki
        występujące w tekście i posortować je w kolejności występowania. Taką pracę wykonuje
        skrypt popular_noun.py
        <label for="%s" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="%s" class="margin-toggle"/>
        <span class="sidenote">Do pobrania z serwisu <a href="https://github.com/pjaskulski/zapiski">github</a>.
        </span>
        .
        </p>

        <p>
        Najpopularniejszy rzeczownik w publikacji Stanisława Bodniaka (<em>Polska a Bałtyk
        za ostatniego Jagiellona</em>) to słowo 'król' występujące w różnych formach ('królów,
        królach, króla, królu, królowi, królem, Króla, Król, król') 435 razy, kolejne
        zaś to 'rok', 'statek', 'straż', 'kapitan', 'komisja', 'okręt', 'żegluga', 'miasto',
        'morze' - co nie jest zaskakujące biorąc pod uwagę tematykę artykułu. Wśród nich
        mamy dwa reprezentujące funkcję: król i kapitan. Przykład fragmentu wyników działania
        skryptu w tabelce poniżej:<br>

        <table>
        <tr><th>Funkcja</th><th>Liczba wystąpień</th><th>Typowe formy</th></tr>
        <tr>
        <td>król</td>
        <td>435</td>
        <td>królów, królach, króla, królu, królowi, królem, Króla, Król, król</td>
        </tr>
        <tr>
        <td>kapitan</td>
        <td>192</td>
        <td>kapitanowie, kapitan, kapitanów, kapitanach, Kapitan, kapitanami, kapitana, kapitanowi, kapitanem, kapitanom</td>
        </tr>
        <tr>
        <td>komisarz</td>
        <td>130</td>
        <td>komisarzom, komisarze, komisarzem, Komisarze, komisarza, komisarzy, komisarz</td>
        </tr>
        </table>
        </p>

        <p>
        Kolejna część pracy należy do człowieka, z otrzymanej listy należy wybrać inne funkcje
        i zawody, znajdziemy tam między innymi funkcje: komisarz, strażnik, kapr, poseł, car, kupiec, żołnierz,
        cesarz, senator, członek, kasztelan, urzędnik, admirał, delegat, mistrz, żeglarz, prezes,
        sekretarz, armator, władca, kanclerz, marynarz, właściciel, dowódca, podkanclerzy,
        namiestnik, książę, ks., starosta, sługa, wojownik, mocodawca, burmistrz. Mając listę
        funkcji można wykorzystując mechanizm Matcher - silnik dopasowywania wzorców biblioteki
        spaCy - wyszukać powiązania funkcji i osób
        <label for="%s" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="%s" class="margin-toggle"/>
        <span class="sidenote"><a href="https://spacy.io/usage/rule-based-matching">Rule-based matching</a>.
        </span>.
        </p>

        <p>
        Wzorzec według którego spaCy dopasowywuje tekst może wyglądać na przykład tak:<br>
        <code>[{"LEMMA": {"IN": lista}}, {"POS":"ADJ", "OP":"?"},{"POS":"PROPN", "OP":"+"}],</code><br>
        gdzie <code>lista</code> jest wyżej wypisaną listą funkcji, <code>LEMMA</code> - oznacza formę podstawową wyrazu,
        <code>"POS":"ADJ"</code> oznacza wyraz będący przymiotnikiem,
        <code>"POS":"PROPN"</code> - oznacza wyraz będący jednostką nazwaną (np. osobą wymienioną z imienia
        i nazwiska), <code>"OP":"?"</code> to operator wskazujący, że poprzedni wzorzec może wystąpić 0 lub 1 raz,
        <code>"OP":"+"</code> zaś jest operatorem oczekującym wystąpienia wzorca co najmniej raz (lub więcej razy).
        </p>

        <p>
        Efektem przetworzenia tekstu badanej publikacji S. Bodniaka przez zestaw podobnych
        reguł (skrypt <code>funkcje_zawody.py</code> - pod tym samym adresem co poprzedni skrypt)
        jest lista wyrażeń zawierających osoby
        i ich funkcje/zawody np.
        <ul>
        <li>"kasztelana chełmińskiego Jerzego Oleskiego"</li>
        <li>"kanclerz Nils Gyllenstierna"</li>
        <li>"admirałem szwedzkim Klausem Flemmingiem"</li>
        <li>"namiestnik Inflant Jan Chodkiewicz"</li>
        <li>"sekretarz królewski Kasper Geschkau"</li>
        <li>"Michała Brunowa kanclerza Gotarda Kettlera"</li>
        </ul>
        </p>

        <p>Możemy stąd uzyskać np. listę występujących w tekście kapitanów i kaprów: Figenow, Gendtrichsen,
        Michał Starosta, Paweł Glasow, Jan Munckenbeck, Kersten Rode, Marcin Śchele, Mateusz Scharping,
        Marek Wilde.
        </p>

        <p>
        Unikalnych znalezisk jest niespełna 90 czyli tylko dla 1/3 osób z indeksu postaci
        stworzonego w poprzedniej zapisce znaleziono ich funkcje lub zawód, ale jest to bardzo
        prosta metoda (pominąłem np. zawody/funkcje występujące mniej niż 10 razy), algortym można zapewne udoskonalić, poza tym te informacje dostajemy
        bez potrzeby ręcznego przeglądania 230 stron publikacji...
        </p>

        </section>
    </article>

    <article>
        <hr>
        <h2 id="2022-10-05">spaCy - jak poprawić wyniki NER czyli Barnim XI to<br> też osoba...
        (<strong>05.10.2022</strong>)</h2>

        <section>
        <p>Rozpoznawanie jednostek nazwanych przeprowadzone na fragmencie publikacji S. Bodniaka
        <label for="%s" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="%s" class="margin-toggle"/>
        <span class="sidenote">
        zob. zapiskę z
        <a href="#2022-10-01">1.10.2022</a>.
        </span>
        dało całkiem niezłe rezultaty w zakresie rozpoznawania osób, zdarzały się jednak takie
        nazwy, które mechanizm pominął. Na przykład 'Barnim XI' - książę pomorski. Czy
        można nauczyć spaCy rozpoznawania takich nietypowych imion? Tak, i nie musi się to
        wiązać z trenowaniem własnego modelu. <strong>EntityRuler</strong> jest komponentem potoku
        przetwarzania spaCy służącym do rozpoznawania jednostek nazwanych poprzez reguły
        <label for="%s" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="%s" class="margin-toggle"/>
        <span class="sidenote">Dokumentacja komponentu
        <a href="https://spacy.io/usage/rule-based-matching#entityruler">EntityRuler</a>.
        </span>
        i może zostać użyty do polepszenia wyników modelu statystycznego NER.
        </p>
        <img src="/static/img_zapiski/ner_barnim_1.png" width="625" alt="tekst bez rozpoznania imienia Barnim"/>

        <p>
        Aby użyć komponentu należy dodać go metodą:<br> <code>nlp.add_pipe("entity_ruler")</code> oraz
        zdefiniować wzorce dla encji, które chcemy
        rozpoznawać w badanym tekście, mogą to być proste wzorce, np. pasujący do naszego przypadku:<br><br>
        <code>pattern = [{"label":"persName", "pattern": "Barnim XI"}]</code><br><br>
        Mogą to także być bardziej zaawansowane wzorce oparte na tokenach:<br><br>
        <code>[{"label": "persName", "pattern": [{"LOWER": "barnim"}, {"LOWER": "wielki"}]}]</code><br><br>
        Korzystamy w tym przypadku z istniejącej w modelu <strong>pl_core_news_md</strong> etykiety <em>persName</em> oznaczającej
        imiona i nazwiska osób.
        </p>

        <p>Powtórne przetworzenie analizowanego fragmentu tekstu powoduje już poprawne
        rozpoznanie wyrażenia 'Barnim XI' jako osoby (label = <em>persName</em>)
        </p>
        <img src="/static/img_zapiski/ner_barnim_2.png" width="625" alt="tekst z rozpoznanym imieniem Barnim"/>

        <p>Komponent EntityRuler potrafi jednak znacznie więcej, możemy z jego pomocą poprosić spaCy
        o rozpoznawanie nowej kategorii obiektów np. funkcji pełnionych przez osoby występujące w tekście.
        Zakładając, że interesują nas funkcje: król, hetman, senator oraz chcemy odnaleźć nie
        tylko dokładnie takie wystąpienia słów (np. 'senator') ale i wersję w liczbie mnogiej ('senatorowie') lub w formie
        odmienionej ('senatorowi') należy użyć bardziej zaawansowanego sposobu definiowana wzorca:<br>
        <code>[{"label":"OCCUPATION", "pattern": [{"LEMMA": {"IN":["hetman","senator","marszałek", "król"]}}]}]</code><br><br>
        Nowa etykieta to <code>'OCCUPATION'</code>, <code>'LEMMA'</code> oznacza formę podstawową słowa, np. dla wyrazów 'senator' i 'senatorowie'
        forma podstawowa będzie równa 'senator', zamiast jednej funkcji np. <code>"LEMMA":"hetman"</code> można we wzorcu wskazać
        od razu całą listę funkcji korzystając z zapisu: <code>{"LEMMA": {"IN":["hetman","senator","marszałek", "król"]}</code>.
        Efekt działania tak zdefiowanego wzorca wraz z całym fragmentem kodu widoczny jest dla zrzucie ekranu poniżej,
        a wszystko to bez trenowania własnego modelu.
        <label for="%s" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="%s" class="margin-toggle"/>
        <span class="sidenote">Notatnik jupytera z kodem ze zrzutu ekranu jest na
        <a href="https://github.com/pjaskulski/zapiski/blob/main/src/entity_ruler_test.ipynb">githubie</a>.
        </span>

        Należy oczywiście zdawać sobie sprawę z ograniczeń, dla języka
        polskiego będą występować problemy z lematyzacją, lemma od słowa <em>podkanclerzy</em> to wg spacy 'podkancler',
        ale już lemma od słowa <em>podkanclerzego</em> to 'podkanclerzy'.
        </p>
        <img src="/static/img_zapiski/entity_ruler_funkcje.png" width="625" alt="EntityRuler - funkcje i zawody"/>

        </section>
    </article>

    <article>
        <hr>
        <h2 id="2022-10-06">Artykuły, kursy, filmy - o NLP w badaniach historycznych (<strong>06.10.2022</strong>)</h2>

        <section>
        <p>
        Internet jest pełen świetnych materiałów na temat NLP, uczenia maszynowego,
        programowania w pythonie czy konkretnych narzędzi i bibliotek,
        w tym miejscu chciałbym gromadzić linki przede wszystkim do tych z nich,
        które dotyczą humanistyki cyfrowej, wykorzystania NLP w badaniach historycznych
        lub były przygotowane z myślą o początkujących humanistach cyfrowych.
        </p>

        <p>
        Kurs <em>Natural Language Processing with spaCy & Python - Course for Beginners</em>
        by Dr. William Mattingly na kanale freeCodeCamp.org:
        <a href="https://www.youtube.com/watch?v=dIUTsFT2MeQ">LINK</a>
        oraz kurs <em>Introduction to spaCy 3</em> w formie podręcznika online:
        <a href="http://spacy.pythonhumanities.com/">LINK</a>
        </p>

        <p>Więcej świetnych filmów i kusrów tego samego autora dostępnych jest na jego
        własnym kanale YT: "Python Tutorials for Digital Humanities"
        <a href="https://www.youtube.com/pythontutorialsfordigitalhumanities">LINK</a>
        oraz na jego stronie np.:
        "Introduction To Named Entity Recognition With a Case Study of Holocaust NER"
        <a href="http://ner.pythonhumanities.com/intro.html">LINK</a>
        </p>

        <p>
        Yvonne Gwerder <em>Named Entity Recognition in Digitized Historical Texts</em>
        (praca magisterska, Universitat Zurich) <a href="https://www.cl.uzh.ch/dam/jcr:cda50f3f-88a3-4e88-a6d9-73deef9df12c/Masterarbeit_YGwerder_FS17.pdf">PDF</a>
        </p>

        <p>
        Felipe Álvarez de Toledo López-Herrera <em>Automated Tagging of Historical, Non-English
        Sources with Named Entity Recognition (NER): A Resource</em>
        <a href="https://blogs.library.duke.edu/data/2020/08/31/automated-tagging-of-historical-non-english-sources-with-named-entity-recognition-ner-a-resource/">LINK</a>
        </p>

        <p>
        Vatsala Nundloll, Robert Smail, Carly Stevens, Gordon Blair
        <em>Automating the extraction of information from a historical text and
        building a linked data model for the domain of ecology and conservation
        science</em>
        <a href="https://www.sciencedirect.com/science/article/pii/S2405844022019983?dgcid=rss_sd_all">LINK</a>
        </p>

        <p>
        Claire Grover, Sharon Givon, Richard Tobin, Julian Ball <em>Named Entity Recognition
        for Digitised Historical Texts</em>
        <a href="https://www.ltg.ed.ac.uk/np/publications/ltg/papers/bopcris-lrec.pdf">PDF</a>
        </p>

        <p>
        Helena Hubková, <em>Named-entity recognition in Czech historical texts</em>
        <a href="http://www.diva-portal.org/smash/get/diva2:1325355/FULLTEXT01.pdf">PDF</a>
        </p>

        <p>
        Blog <em>NLP for Historical Texts</em> <a href="https://nlphist.hypotheses.org/">https://nlphist.hypotheses.org/</a>
        prowadzony przez Michaela Piotrowskiego, autora książki <em>Natural Language
        Processing for Historical Texts</em> (2012) <a href="https://link.springer.com/book/10.1007/978-3-031-02146-6">LINK</a>
        </p>

        <p>
        Asarsa Kunal <em>Analysis of named entity recognition & entity linking
        in historical text</em> (Masters theses, Northeastern University) <a href="https://repository.library.northeastern.edu/files/neu:cj82nh21q/fulltext.pdf">PDF</a>
        </p>

        <p>Kimmo Kettunen, Eetu Mäkelä, Teemu Ruokolainen, Juha Kuokkala, Laura Löfberg
        <em>Old Content and Modern Tools – Searching Named Entities in a Finnish OCRed
        Historical Newspaper Collection 1771–1910</em>
        <a href="http://www.digitalhumanities.org/dhq/vol/11/3/000333/000333.html">LINK</a>
        </p>

        </section>
    </article>

    <article>
        <hr>
        <h2 id="2022-10-07">Anotacja tekstów w Doccano i trenowanie własnego<br> modelu (<strong>07.10.2022</strong>)</h2>

        <section>
        <p>
        Wykorzystanie reguł do wyszukiwania encji nazwanych poprzez komponent EntityRuler
        jest całkiem skutecznym sposobem poprawienia wyników NER, jeżeli w wykorzystywanym
        modelu brakuje np. jednej etykiety i możemy łatwo uzupełnić ten brak mając zestaw
        słów kluczowych i tworząc odpowienie reguły. W trakcie prac nad tekstami spotkamy się
        jednak na pewno z bardziej skomplikowanymi przypadkami, a wówczas niezbędne stanie
        się wytrenowanie własnego modelu dopasowanego do specyfiki naszych problemów. Przed
        procesem trenowania niezbędne są jednak trzy wstępne etapy: należy zgromadzić zestaw
        danych (tekstów) do uczenia, przygotować anotacje tych tekstów oraz zdecydować czy
        będziemy trenować własny model od zera, czy też dotrenowywać istniejący model. Krok
        drugi wiąże się też z wyborem oprogramowania do antoacji oraz przygotowaniem jej
        wyników do formy akceptowalnej przez spaCy.
        </p>

        <p>
        Celem testu będzie przygotowanie danych a następnie wytrenowanie modelu rozpoznającego
        zawody/funkcje (dla uproszczenia będzie to mały model trenowany 'od zera'). Próbką tekstów
        do anotacji w naszym teście będzie zbiór fragmentów artykułu S. Bodniaka,
        <label for="%s" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="%s" class="margin-toggle"/>
        <span class="sidenote">Zob. zapiskę z
        <a href="#2022-10-01">1.10.2022</a>.
        </span>
        pochodzących z różnych części tej publikacji, lecz zawsze zawierających określenia
        zawierające nazwy zawodów lub funkcji. Zbiór liczy w sumie 105 zdań, czyli parę procent
        całego tekstu, zapewne dla osiągnięcia naprawdę dobrych wyników powinien być 
        wyraźnie większy (wspomina się o minimum 100 przykładach dla każdej uczonej encji).
        Najlepszą aplikacją do anotacji współpracującą z spaCy jest z pewnoścą Prodigy,
        narzędzie przygotowane przez autorów spaCy - firmę Explosion. Jest to jednak
        oprogramowanie komercyjne i choć w realnej sytuacji z pewnością warto w nie
        zainwestować (ceny zaczynają się od 390 USD za licencję) to w naszym teście
        użyjemy narzędzia dostępnego bezpłatnie. Istnieje ich co najmniej kilka np.
        <a href="https://tecoholic.github.io/ner-annotator/">ner-annotator</a>,
        <a href="(https://github.com/d5555/TagEditor">TagEditor</a> (aplikacja desktopowa
        dla systemu Windows),
        <a href="https://brat.nlplab.org/">brat</a>,
        <a href="https://inception-project.github.io/">INCEpTION</a> czy
        <a href="https://github.com/doccano/doccano">doccano</a>. Każde z nich na pewno
        dobrze się sprawdzi do wprowadzenia etykiet, jednak najlepsze wrażenie zrobiło
        na mnie doccano, aplikacja webowa, którą można w prosty sposób zainstalować
        lokalnie korzystając z dockera. Opis instalacji jest na stronie projektu,
        polecam ścieżkę z wykorzystaniem docker-compose.
        <label for="%s" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="%s" class="margin-toggle"/>
        <span class="sidenote">doccano - instalacja poprzez
        <a href="https://github.com/doccano/doccano#docker-compose">docker-compose</a>.
        </span>
        </p>

        <p>
        Praca w aplikacji doccano zaczyna się od zalogowania na nasze konto (podczas
        instalacji zakładany jest użytkownik będący administratorem, wraz z hasłem,
        później można utworzyć dodatkowych użytkowników nadając im role np. anotatora,
        czy osoby akceptującej anotacje) i utworzenia projektu. Zakładając nowy projekt
        należy wskazać typ projektu (rodzaj zadania), w naszym przypadku chcemy 
        rozpoznawać jednostki nazwane (NER) należy więc wybrać <em>sequence labeling</em>.
        </p>
        <img src="/static/img_zapiski/doccano_lista_projektow.png" width="600" alt="doccano lista projektów"/>

        <p>
        Do utworzonego projektu można zaimportować dane do anotacji (menu Dataset), doccano
        akceptuje różne pliki tekstowe (lub zestawy plików), zaimportowane dane będą widoczne
        w formie tabelki.
        </p>
        <img src="/static/img_zapiski/doccano_lista.png" width="600" alt="doccano lista dokumentów"/>

        <p>
        Przed rozpoczęciem anotowania niezbędne jest jeszcze utworzenie etykiet, którymi
        będzie oznaczany tekst, biorąc pod uwagę cel testu jedyną używaną etykietą
        będzie <em>occupation</em> na oznaczenie zawodu lub funkcji pełnionej przez
        postacie występujące w tekście.
        </p>
        <img src="/static/img_zapiski/doccano_tworzenie_etykiet.png" width="600" alt="doccano tworzenie etykiet"/>

        <p>Sam proces anotacji w doccano jest bardzo zbliżony do pracy w innych apliakacjach
        tego typu, zaznaczamy słowo lub wyrażenie, program wyświetla wówczas podręczne menu
        z którego można wybrać właściwą etykietę, każda z etykiet ma swój kolor (definiowany
        przez użytkownka na etapie tworzenia etykiet) i takim kolorem doccano podreśla oznaczony
        fragment tekstu (wyświetla też nazwę etykiety).
        </p>
        <img src="/static/img_zapiski/doccano_w_trakcie_anotacji.png" width="600" alt="doccano w trakcie anotacji"/>

        <p>
        Po zakończonym procesie anotacji można otagowany zbiór danych wyeksportować do
        formatu JSONL, w którym znajduje się anotowany tekst oraz etykiety i ich pozycje
        w tekście. Niestety nie jest to gotowy plik, który możemy użyć do trenowania modelu
        z użyciem spaCy. W obecnej wersji (3.4 w momencie pisania zapiski) spaCy oczekuje
        pliku binarnego z przyjętym zwyczajowo rozszerzeniem *.spacy (format DocBin). Na
        szczęście kowersja pliku *.jsonl na *.spacy nie jest skomplikowana, można do tego
        celu użyć przykładowego skryptu <code>doccano2spacy.py</code>, który wywoływany
        jest z nazwą pliku jsonl jako jedynym parametrem.
        <label for="%s" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="%s" class="margin-toggle"/>
        <span class="sidenote">Skrypt można pobrać z repozytorium
        <a href="https://github.com/pjaskulski/zapiski/blob/main/src/doccano2spacy.py">github</a>.
        </span>
        </p>
        <img src="/static/img_zapiski/doccano_fragment_pliku_jsonl.png" width="600" alt="doccano fragment pliku jsonl"/>

        <p>
        Mając zbiór uczący można przystąpić do trenowania nowego modelu. Proces trenowania
        wywoływany jest z linii komend poleceniem spacy train, należy też podać nazwę pliku
        konfiguracyjnego (np. config.cfg) z parametrami uczenia, spacy ułatwi jego
        utworzenie z domyślnymi parametrami, procedura opisana jest na stronie
        <a href="https://spacy.io/usage/training#quickstart">spacy.io</a> (tak, opis jest
        długi - to jest skomplikowane!). Należy też podać ścieżkę do zbioru uczącego
        i walidacyjnego (aby zachować reguły gry należałoby także przygotować zbiór walidujący,
        tu dla uproszczenia skorzystam z kopii zbioru uczącego). Parametr <code>--output</code>
        polecenia wskazuje gdzie będą zapisane wytrenowane modele (spacy zapisuje ostatni
        i najlepszy z modeli).<br>
        <code>python -m spacy train config.cfg --output ./output --paths.train ./bodniak.spacy --paths.dev ./bodniak.spacy</code><br>
        Podczas działania (dla większych zbiorów czas obliczeń może być spory, w tym przypadku
        będzie to kilkadziesiąt sekund) wyświetlany jest stan kolejnch iteracji uczenia
        (zob. zrzut ekranu poniżej).
        </p>
        <img src="/static/img_zapiski/proces_trenowania_modelu.png" width="600" alt="proces trenowania modelu"/>

        <p>
        Po zakończeniu trenowania można przystąpić do weryfikacji działania modelu.
        Wykorzystamy do tego środowisko <strong>jupyter</strong> i mechanizm wizualizacji
        <strong>displacy</strong>. Wczytanie nowego modelu w spaCy odbywa się podobnie
        jak modelu standardowego, z tym że zamiast nazwy należy podać ścieżkę do katalogu
        z zapisanym modelem: <code>nlp = spacy.load('../output/model-best/')</code>. Wynik
        działania na spreparowanym zdaniu, w którym występują nazwy funkcji/zawodów widać
        na załączonym niżej zrzucie ekranu z fragmentem notebooka aplikacji jupyter.
        Można zauważyć poprawne rozpoznanie słów 'król', 'kapitan', 'kanclerzem', jednak
        wyrażenie 'podstarościm' pozostało niezauważone, najwyraźniej model nie zdołał się
        go nauczyć, w materiale uczącym występowało ono faktycznie niezbyt często.
        </p>
        <img src="/static/img_zapiski/test_modelu_ner.png" width="650" alt="test modelu ner"/>

        <p>
        I jeszcze dwa przydatne linki, do dokumentacji aplikacji doccano:
        <a href="https://doccano.github.io/doccano/">LINK</a>
        oraz do artykułu, który dużo obszerniej opisuje tą aplikację:
        <a href="https://towardsdatascience.com/doccano-a-tool-to-annotate-text-data-to-train-custom-nlp-models-f4e34ad139c3">LINK</a>.
        </p>
        </section>

    </article>

{{end}}
