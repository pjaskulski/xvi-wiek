{{template "base" .}}

{{define "title"}}XVI wiek - zapiski{{end}}
{{define "meta_description"}}"Zapiski (podstrona serwisu XVI-wiek.pl) - notatki, python, NLP, humanistyka cyfrowa"{{end}}
{{define "meta_title"}}"XVI wiek - zapiski"{{end}}
{{define "canonical"}}<link rel="canonical" href="https://xvi-wiek.pl/zapiski" />{{end}}

{{define "main"}}

    <h1 id="tufte-css">Zapiski</h1>

    <div class="preface">Zapiski, notatki i ciekawostki dotyczące programowania związanego
    z historią, archeologią czy tzw. humanistyką cyfrową. Czyli o praktycznym wykorzystaniu
    różnych narzędzi i bibliotek.
    </div>
    <p></p>

    <div>
    <p>
    Spis treści:<br><br>
    <strong>2022-09-25</strong> <a href="#2022-09-25">OpenRefine - własny serwis rekoncyliacji/uspójniania</a><br>
    <strong>2022-10-01</strong> <a href="#2022-10-01">spaCy, NER i półautomatyczne tworzenie indeksu postaci</a><br>
    <strong>2022-10-03</strong> <a href="#2022-10-03">spaCy, POS i Matcher - czyli do czego przydają się rzeczowniki</a><br>
    <strong>2022-10-05</strong> <a href="#2022-10-05">spaCy - jak poprawić wyniki NER czyli Barnim XI to też osoba...</a><br>
    <strong>2022-10-06</strong> <a href="#2022-10-06">Artykuły, kursy, filmy - o NLP w badaniach historycznych<a/><br>
    <strong>2022-10-07</strong> <a href="#2022-10-07">Anotacja tekstów w Doccano i trenowanie własnego modelu<a/><br>
    <strong>2022-10-10</strong> <a href="#2022-10-10">Skrypty w poszukiwaniu wydarzeń</a><br>
    <strong>2022-10-17</strong> <a href="#2022-10-17">spaCy, NEL - łączenie rozpoznanych encji z wikidata.org</a><br>
    <strong>2022-10-22</strong> <a href="#2022-10-22">spaCy i automatyczne tagowanie encji w TEI Publisherze</a><br>
    <strong>2022-10-30</strong> <a href="#2022-10-30">spaCy i koreferencje, czyli Jerzy bawił we Włoszech a <strong>jego</strong> włości popadały w ruinę</a><br>
    <strong>2023-01-??</strong> <a href="#2023-01-??">Kraken, czyli HTR to jest to</a><br>
    <strong>2023-01-29</strong> <a href="#2023-01-29">Dżin z czarodziejskiej lampy czyli GPT 3.5</a><br>
    <strong>2023-06-08</strong> <a href="#2023-06-08">Podręczny OCR. Jak odczytywać fragmenty tekstu z obrazów</a><br>
    <strong>2023-07-02</strong> <a href="#2023-07-02">Duże modele językowe a grafy wiedzy</a><br>
    </p>
    <p>Tłumaczenia i nowe wpisy pojawiają się też na platformie
    <a href="https://medium.com/@piotr.jaskulski">Medium</a>, na przykład:<br>
    "Extracting information from historical studies. How to ask GPT?"
    <a href="https://medium.com/@piotr.jaskulski/extracting-information-from-historical-studies-how-to-ask-gpt-1ee3895fd126">
    link</a>.
    </p>
    </div>

    <article>
        <hr>
        <h2 id="2022-09-25">OpenRefine - własny serwis rekoncyliacji/uspójniania (<strong>25.09.2022</strong>)</h2>

        <section>
            <p>
                <a href="https://openrefine.org/" target="_blank" rel="noopener">OpenRefine</a>
                jest popularnym narzędziem do oczyszczania i przekształcania danych.

                <label for="1-1" class="margin-toggle sidenote-number"></label>
                <input type="checkbox" id="1-1" class="margin-toggle"/>
                <span class="sidenote">Polecam artykuły na temat OpenRefine:
                <a href="https://www.pilsudski.org/pl/instytut-cyfrowy/pracownia-digitalizacji/blog/869-oczyszczanie-danych-z-uzyciem-openrefine" target="_blank" rel="noopener">
                Oczyszczanie danych z użyciem OpenRefine
                </a>
                oraz
                <a href="http://programminghistorian.org/en/lessons/cleaning-data-with-openrefine" target="_blank" rel="noopener">
                Cleaning Data with OpenRefine
                </a>
                </span>

                Jedną z jego ciekawszych funkcji jest możliwość rekoncyliacji danych
                przy wykorzystaniu zewnętrznych serwisów udostępniających dane do uspójniania. Często zdarzają 
                się w danych historycznych różnorakie sposoby zapisu imion i nazwisk postaci, lub nazw geograficznych.
                Nazwy miejscowości mogą występować w brzmieniu współczesnym, w formie używanej w XVI wieku,
                lub po prostu w formie błędnie zapisanej w źródle. Mechanizm uspójniania w OpenRefine pozwala
                uzgodnić nazwę występującą w naszych danych z nazwą pochodzącą z pewnego źródła. Np. powiązać
                "Zbigniewa Oleśnickiego" z identyfikatorem z bazy VIAF, lub powiązać miejscowość Brzozowa (występująca
                w źródłach także jako Brosoua) z identyfikatorem z Atlasu Historycznego Polski (Brzozowa_sdc_krk).
            </p>
            <p> Problemem może być znalezienie odpowiedniego serwisu rekoncyliacji. O ile popularne źródła
                w rodzaju VIAF czy wikidata.org są dostępne

                <label for="1-2" class="margin-toggle sidenote-number"></label>
                <input type="checkbox" id="1-2" class="margin-toggle"/>
                <span class="sidenote">Lista
                <a href="https://github.com/OpenRefine/OpenRefine/wiki/Reconcilable-Data-Sources" target="_blank" rel="noopener">dostępnych</a>
                serwerów rekoncyliacji.
                </span>

                , o tyle uspójnianie z bazą miejscowości AHP lub naszą lokalną
                bazą osób może wymagać uruchomienia własnego serwisu. Nie jest to jednak takie trudne, istnieją
                bowiem gotowe narzędzia, które można wykorzystać do tego celu. Jednym z nich jest
                <strong>Reconcile-csv</strong> udostępniony (na otwartej licencji BSD-2) na stronie
                <a href="http://okfnlabs.org/reconcile-csv/" target="_blank" rel="noopener">Open Knowledge Lab</a>.
                Program ten pozwala na dopasowanie szukanej przez nas nazwy do nazw w swojej bazie poprzez
                mechanizm przybliżonego (rozmytego) porównywania (fuzzy matching), nazwy nie muszą
                być identyczne, mogą być podobne, program będzie wówczas proponował listę zbliżonych
                do podanej nazw wraz ze współczynnikiem podobieństwa. Bazą dla Reconcile-csv
                jest plik tekstowy w formacie CSV.  
            </p>

            <p>
            Program jest plikiem *.jar,
            (warto pobrać z <a href="https://github.com/okfn/reconcile-csv" target="_blank" rel="noopener">githuba</a>
            też plik index.html.tpl, który jest szablonem
            wyświetlanym jako główna strona serwisu) do uruchomienia potrzebuje więc zainstalowanej Javy (JRE),
            potrzebny jest oczywiście plik CSV z danymi (plik w którym każdy wiersz jest rekordem
            danych, pola rozdzielone są przecinkami a pierwszy wiersz zawiera nazwy kolumn).
            </p>

            <p>
            Polecenie uruchamiające serwer:<br>
            <code>
                java -Xmx2g -jar reconcile-csv-0.1.2.jar plik.csv search_column id_column
            </code>
            </p>

            <p>
            gdzie <code>plik.csv</code> to nazwa naszego pliku z danymi, <code>search_columm</code>
            to nazwa pola w pliku z danymi, które będzie służyło do rekoncyliacji w OpenRefine,
            <code>id_column</code> to nazwa pola w pliku z danymi, które zawiera jednoznaczny identyfikator danych.
            </p>

            <p>Po uruchomieniu serwis dostępny jest pod adresem <em>http://localhost:8000/reconcile</em>,
            i taki adres należy wprowadzić w OpenRefine (zwraca on zawartość w formacie json, czytelną dla
            programu - odbiorcy danych z serwisu). Adres główny serwisu (<em>http://localhost:8000</em>)
            wyświetli o nim podstawowe informacje w formie czytelnej dla człowieka.
            </p>
            <img src="/static/img_zapiski/reconcile_csv_01.png" width="500" alt="serwis reconcile-csv"/>

            <p>
            Lokalny serwis rekoncyliacji podłącza się do OpenRefine podobnie jak serwisy zewnętrzne,
            po uruchomieniu rekoncyliacji na wybranej kolumnie danych przycisk Add Standard Service wyświetli
            okienko w którym można podać adres serwisu, zaakceptowany serwis pojawi się na liście.
            <label for="1-3" class="margin-toggle sidenote-number"></label>
            <input type="checkbox" id="1-3" class="margin-toggle"/>
            <span class="sidenote">Standardowa nazwa serwisu będzie różnić się od pokazanej na
            ilustracji i wygląda tak: 'CSV Reconciliation service'. Jest niestety zaszyta w kodzie źródłowym
            Reconcile-csv i jej zmiana wymaga rekompilacji programu (zob. dalszą część artykułu).
            </span>
            </p>
            <img src="/static/img_zapiski/reconcile_csv_04.png" width="500" alt="add standard service"/>

            <p>
            Można wówczas rozpocząć proces uspójniania. Dane które według mechanizmu zostały pewnie dopasowane
            (współczynnik > 0.85) zostaną od razu przypisane do komórek kolumny dla której przeprowadzamy
            rekoncyliację, w innych przypadkach wyświetlona zostanie lista wraz z wartościami dopasowania.
            Dla każdej z pozycji można wyświetlić informacje z pliku csv będącego źródłem danych serwisu, na
            przykładzie poniżej są to dane miejscowości z Atlasu Historycznego Polski.
            <label for="1-4" class="margin-toggle sidenote-number"></label>
            <input type="checkbox" id="1-4" class="margin-toggle"/>
            <span class="sidenote">Dostępne publicznie na licencji CC BY-ND 4.0:
            <a href="https://data.atlasfontium.pl/documents/202?_ga=2.122616834.865899583.1640103019-2000553280.1624037407" target="_blank" rel="noopener">
            AHP 2.0</a> (IH PAN).
            </span>
            </p>
            <img src="/static/img_zapiski/reconcile_csv_11.png" width="500" alt="reconcile_view"/>

            <p><strong>Jak zmodyfikować program reconcile-csv</strong>:
            zarówno zmiana nazwy serwisu (nazwy wyświetlanej w OpenRefine), jak i dostosowanie programu
            Reconcile-csv do pracy na serwerze, wymaga zmian w jego kodzie źródłowym
            <label for="1-5" class="margin-toggle sidenote-number"></label>
            <input type="checkbox" id="1-5" class="margin-toggle"/>
            <span class="sidenote">Repozytorium aplikacji w serwisie
            <a href="https://github.com/okfn/reconcile-csv">github</a>.
            </span>
            a w związku z tym pewnych umiejętności 'programistycznych'.
            Sama zmiana nazwy serwisu wymaga jednak tylko modyfikacji 50 wiersza pliku
            /src/reconcile_csv/core.clj: <code> {:name "CSV Reconciliation service"</code>,
            zmiany tekstu <em>"CSV Reconciliation service"</em> na nowy i rekompilacji programu.
            Aplikacja została stworzona w języku clojure. Najłatwiejszym sposobem zarządzania i kompilacji projektu
            w języku clojure jest użycie systemu
            <a href="https://leiningen.org/" target="_blank" rel="noopener">Leiningen</a>, który w systemie
            Ubuntu można zainstalować poleceniem: <code>sudo apt install leiningen</code>.
            Rekompilacja projektu Reconcile-csv i budowa nowego pliku *.jar sprowadza się do
            uruchomienia polecenia: <code>lein uberjar</code> w katalogu z plikiem project.cli,
            skompilowany plik *.jar powinien znajdować się w podkatalogu <code>/target</code>.
            </p>

            <p>
            Modyfikacja kodu i rekompilacja za każdym razem kiedy chcemy zmienić np. nazwę serwisu
            rekoncyliacji wyświetlaną w OpenRefine może być jednak dość niewygodna. Dlatego przygotowałem małą
            modyfikację programu reconcile-cvs która obsługuje cztery dodatkowe parametry uruchamiania z linii komend:
            <code>adres_serwera</code>, <code>port</code>, <code>nazwa_serwisu_rekoncyliacji</code> i <code>nazwa_typu</code>.
            <label for="1-6" class="margin-toggle sidenote-number"></label>
            <input type="checkbox" id="1-6" class="margin-toggle"/>
            <span class="sidenote">W razie potrzeby wystawiania naszego serwisu na serwerze dostępnym w internecie
            można to zrobić za pośrednictwem serwera nginx, który będzie służył jako proxy dla wbudowanego w reconlice-csv
            jetty, wówczas reconcile-csv będzie pracował pod adresem np. localhost:8080, a o obsługę żądań zewnętrznych
            zadba nginx.
            </span>
            Dodatkowo serwis wyświetla trochę bardziej czytelny podgląd danych z pliku csv, podczas
            uspójniania (widoczne ramki tabeli).
            <img src="/static/img_zapiski/reconcile_csv_table.jpg" width="500" alt="reconcile_view"/>
            </p>

            <p>Przykładowe wywołanie:<br>
            <code>
             java -Xmx2g -jar reconcile-csv-0.1.2.jar plik.csv search_column id_column "http://moj_serwer.org" "80" "Miasta-Osoby Serwis Rekoncyliacji" "miasta_osoby"
            </code>
            <p>
            <p>
            lub (obecnie port inny niż 80 trzeba podać także w adresie serwera):<br>
            <code>
             java -Xmx2g -jar reconcile-csv-0.1.2.jar plik.csv search_column id_column "http://localhost:8000" "8000" "Miasta-Osoby Serwis Rekoncyliacji" "miasta_osoby"
            </code>
            </p>

            <p>Kod źródłowy modyfikacji można pobrać z <a href="https://github.com/pjaskulski/reconcile-csv" target="_blank" rel="noopener">github</a>,
            skompilowany plik jar również: <a href="https://github.com/pjaskulski/reconcile-csv/releases" target="_blank" rel="noopener">releases</a>.
            </p>
        </section>
    </article>

    <article>
        <hr>
        <h2 id="2022-10-01">spaCy, NER i półautomatyczne tworzenie indeksu postaci (<strong>01.10.2022</strong>)</h2>

        <section>

        <p>Postęp rozwoju metod przetwarzania języka naturalnego (NLP) w ciągu ostatnich kilku lat pozwala
        na coraz szersze ich użycie, także w odniesieniu do tekstów w języku polskim. Jedną z technik NLP
        jest rozpoznawanie jednostek nazwanych (NER - named entity recogition), w czystym, pozbawionym struktury
        tekście poddanym analizie NER można oznaczyć np. osoby, miejsca czy daty. Do czego może się to przydać w przypadku
        publikacji historycznych? Jednym z najprostszych przypadków użycia jaki przychodzi do głowy jest stworzenie
        indeksu osób dla publikacji gdzie takiego indeksu brakuje.</p>

        <p>Do testu posłuży artykuł (właściwie książka, sądząc po objętości) Stanisława Bodniaka, "Polska a Bałtyk za ostatniego z Jagiellona"
        wydana w 1946 roku (<em>Pamiętnik Biblioteki Kórnickiej</em> 3, 42-276, 1939-46).
        <label for="2-1" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="2-1" class="margin-toggle"/>
        <span class="sidenote">
        Dzieło to ma zresztą ciekawą historię gdyż pierwszy druk i rękopis zostały utracone
        w wyniku działań wojennych, autor z zachowanych korekt i notatek
        odtworzył i wydał je po wojnie.
        </span>
        Publikacja została zdigitalizowana przez Muzeum Historii Polski i udostępniona w bazie
        bazhum.muzhp.pl w formie pdf.
        </p>
        <p>
        Sytuacja jednak nie jest idealna - pdf nie ma indeksu (nie wiem czy jest w papierowym
        wydaniu), ma warstwę OCR więc jest przeszukiwalny, jednak jakość OCR nie jest perfekcyjna,
        w tekście pojawiają się np. przypadkowe spacje wewnątrz słów. Aby to poprawić można
        proces ocr przeprowadzić powtórnie.
        <label for="2-2" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="2-2" class="margin-toggle"/>
        <span class="sidenote">Ponieważ na codzień pracuję w systemie Linux, narzędzia których używam związane są z
        tym systemem, jednak wiele z nich pracuje także pod Windows/Mac lub istnieją odpowiedniki
        dla tych środowisk.
        </span>
        </p>
        <img src="/static/img_zapiski/tekst_z_warstwy_ocr_pdfa.png" width="500" alt="reconcile_view"/>

        <p>Ciekawym narzędziem do wprowadzania warstwy tekstowej ocr do plików pdf jest
        <a href="https://ocrmypdf.readthedocs.io/en/latest/introduction.html" target="_blank" rel="noopener">ocrmypdf</a>, może
        on rówież nadpisać istniejącą warstwę tekstową czy zapisać rozpoznany przez ocr na nowo
        tekst w osobnym pliku tekstowym (warstwa tekstowa zapisana przez ten program w pdf
        również nie była rewelacyjna, ale sam plik z tekstem okazał się niezły i nadawał się
        do dalszej obróbki). Silnikiem OCR używanym przez omawianą aplikację jest Tesseract,
        uważany za najlepszy mechanizm OCR open source.
        </p>
        <p>
        Polecenie uruchamiające przetwarzanie pdf-a wygląda tak:<br>
        <code>
        ocrmypdf -l pol --force-ocr --sidecar output.txt Bodniak_input.pdf Bodniak_output.pdf
        </code>
        </p>

        <p>
        gdzie opcja: <code>-l pol</code> odpowiada za obsługę języka polskiego,
        <code>--force-ocr</code> wymusza nadpisanie istniejącej warstwy ocr w pdf,
        zaś opcja <code>--sidecar output.txt</code> powoduje zapisanie tekstu z pdf
        w pliku tekstowym. Nazwy plików pdf na końcu to odpowiednio oryginalny plik
        oraz wyjściowy zmodyfikowany plik pdf.
        </p>

        <p>
        Oryginalny plik pdf pobrany z repozytorium bazhum.muzhp.pl może wymagać wstępnego
        przetworzenia (program ocrmypdf zgłasza błąd "EncryptedPdfError: Input PDF is encrypted.
        The encryption must be removed to perform OCR"), służy do tego program qpdf uruchamiany
        z linii komend: <code>qpdf --decrypt plik_wejściowy.pdf plik wyjściowy.pdf</code>.
        </p>

        <p>
        Innym narzędziem, które może posłużyć do wydobycia tekstu z pliku pdf jest program
        gImageReader będący graficzną nakładką na silnik OCR Tesseract, oprócz samego procesu
        OCR ma on dodatkowe funkcje pozwalające usunąć znaki końca wiersza ze środka akapitów
        oraz połączyć wyrazy podzielone między wiersze (komercyjne programy OCR mają z pewnością
        dużo większe możliwości w zakresie oczyszczania i poprawiania tekstu po procesie OCR).
        </p>
        <img src="/static/img_zapiski/tekst_po_ocr_gImageReader_Tesseract.png" width="500" alt="tekst_gimage_reader"/>

        <p>Mając plik z tekstem (z zachowanymi numerami stron, które zamieniłem na tagi w postaci
        <code>[PAGE: 45]</code> na początku każdej strony) możemy poddać go próbnej analizie NER.
        Istnieje wiele narzędzi i modeli, które można wykorzystać do tego celu, dostępna
        jest np. kolekcja narzędzi projektu CLARIN-PL (analiza NER do przetestowania także:
        <a href="https://ws.clarin-pl.eu/ner" target="_blank" rel="noopener">on-line</a> z wykorzystaniem narzędzia Liner2). W tym teście skorzystam jednak z
        popularnej biblioteki NLP: <a href="https://spacy.io/" target="_blank" rel="noopener">spaCy</a> i skryptów w języku Python.
        </p>

        <p>Najwygodniejszym sposobem testowania różnych rozwiązań w Pythonie jest środowisko
        <a href="https://jupyter.org/" target="_blank" rel="noopener">Jupyter Notebook</a>
        Sama instalacja i konfiguracja Pythona, Jupytera i spaCy mogłaby być tematem osobnej zapiski, ale
        procudury ich instalacji są przystępnie opisane na podanych wyżej stronach.
        </p>

        <p>
        Po uruchomieniu środowiska Jupyter i utworzeniu nowego notatnika należy zaimportować
        bibliotekę spaCy oraz dodatkowo mechanizm wizualizacji displacy. Kolejnym krokiem będzie
        załadowanie tekstu do analizy oraz modelu dla języka polskiego. Standardowo spaCy oferuje
        3 modele (zob. <a href="https://spacy.io/models/pl" target="_blank" rel="noopener">https://spacy.io/models/pl</a>), mały,
        średni i duży, różniące się wielkością (od 20 do 500 MB) i dokładnością. Do celów tego testu
        wystarczy model średni 'pl_core_news_md' (<em>Polish pipeline optimized for CPU. Components:
        tok2vec, morphologizer, parser, lemmatizer (trainable_lemmatizer), tagger, senter, ner.</em>).
        </p>

        <p>
        <code>
        import spacy<br>
        from spacy import displacy<br><br>

        nlp = spacy.load('pl_core_news_md')<br>
        with open('../data/bodniak_baltyk_small.txt', 'r', encoding='utf-8') as f:<br>
            text = f.read()<br><br>

        doc = nlp(text)<br>
        displacy.render(doc, style='ent')<br>
        </code>
        </p>

        <p>
        Kod podany powyżej przeprowadza analizę NER wczytanego tekstu (jest to fragment
        artykułu S. Bodniaka, str. 43) oraz wyświetla wynik w bardzo przystępnej wizualnie formie,
        gdzie znalezione jednostki nazwane (osoby, daty i miejsca) oznaczone są kolorami
        i etykietą ('persName', 'date', 'placeName'). Jak widać mechanizm radzi sobie całkiem nieźle z typowymi przypadkami
        osób, znajduje określenia temporalne i miejscowości.
        </p>
        <img src="/static/img_zapiski/jupyter_spacy.png" width="685" alt="jupyter_spacy"/>

        <p>Niektóre z postaci, np. Barnim XI (książę pomorski, w dolnej części tekstu na
        załączonym zrzucie ekranu) pozostają jednak nierozpoznane, jako osoby rozpoznawane są natomiast
        błędnie wyrażenia typu: 'Odstraszała' czy 'Podniosłyby', model użyty do testu był
        przygotowany na podstawie bardziej współczesnych tekstów stąd imion i innych określeń
        używanych historyczne po prostu może nie znać. Analizowany tekst dotyczy XVI wieku, ale gdyby
        występowały w nim postacie średniowieczne typu 'Jan z Goźlic' jako osoba zostałoby
        uznane prawdopodobnie tylko imię Jan a nie całe wyrażenie. Zdarzają się też błędne
        rozpoznania miejsc jako osób itp. Czy można model 'przekonać' do bardziej poprawnego rozpoznawania
        takich określeń? Oczywiście, istnieją także zapewne lepsze modele, ale jest to temat na
        inny wpis.</p>

        <p>
        Mając tekst z informacjami o numerach stronach, możliwość rozpoznania postaci analizą NER,
        możemy przygotować ich indeks, posłuży do tego skrypt pythonowy create_index.py
        <label for="2-3" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="2-3" class="margin-toggle"/>
        <span class="sidenote">Do pobrania z serwisu <a href="https://github.com/pjaskulski/zapiski" target="_blank" rel="noopener">github</a>.
        </span>
        w którym najpierw wczytywana jest treść każdej strony,
        wykonywana analiza NER, wyniki dopisywanie do słownika wyników, na koniec wyświetlana
        jest posortowana lista znalezionych postaci (266 osób, z tym że niektóre występują czasem
        podwójnie lub potrójnie z powodu np. literówek czy błędnie ustalonej formy podstawowej
        imienia i nazwiska) wraz z numerami stron. Lista nie jest z pewnością dokładna, wymaga
        oczyszczenia przez człowieka (występują wspomniane już wyżej błędne rozpoznania, problemy
        z odmianą imion i nazwisk, jakość ocr też wpływa na błędy wynikające z literówek) jednak
        z pewnością jest dużym ułatwieniem przy tworzeniu indeksu, no i jej przygotowaniem zajął
        się komputer, dając człowiekowi czas na zadania, których komputer na dziś nie jest w stanie
        się podjąć. Poniżej fragment przygotowanego indeksu po wstępnym oczyszczeniu.
        </p>

        <p>
        <code>
        Achacy Czema [262]<br>
        Adam Konarski [190, 273]<br>
        Adolf (holsztyński) [157]<br>
        Albrecht Giese [88, 120, 171]<br>
        Albrecht Wilkowski [116]<br>
        Albrecht (Hohenzollern) [43, 44, 51, 52, 56, 57, 58, 70, 134]<br>
        Aleksander Połubieński [216]<br>
        Andrzej Swarożyński [74, 81, 84, 104, 274]<br>
        Antoni Angela [44]<br>
        Bajerski [93, 94]<br>
        Barnim XI [126, 187]<br>
        Bąkowski [138]<br>
        Birsen [46]<br>
        Bogusław XIII [187]<br>
        Boleman [177, 178]<br>
        Chodkiewicz [101, 191, 207, 228, 263, 270, 271]<br>
        [...]<br>
        Łukasz Górnicki [43]<br>
        Łukasz Podoski [190, 222, 273]<br>
        Maciej Scharping [58, 60, 67, 74, 83]<br>
        Maciej Strubicz [77, 271]<br>
        </code>
        </p>

        </section>
    </article>

    <article>
        <hr>
        <h2 id="2022-10-03">spaCy, POS i Matcher - czyli do czego przydają się rzeczowniki
        (<strong>03.10.2022</strong>)</h2>

        <section>
        <p>Mając indeks postaci występujących w publikacji
        <label for="3-1" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="3-1" class="margin-toggle"/>
        <span class="sidenote">Zob. poprzednią <a href="#2022-10-01">zapiskę</a>.
        </span>
        wiemy kto występuje w tekście, ale
        czy wiemy kim był? W przypadku osób powszechnie znanych kojarzymy, że Zygmunt August
        był królem, Albrecht Hohenzollern władcą Prus, ale jakie funkcje pełnili, jakich zawodów
        byli przedstawicielami ci pozostali, mniej znani? Czy analizy NLP mogą  nam pomóc w
        odpowiedzi na te pytania? Tak i to nawet najbardziej podstawowe metody, jak określenia
        części mowy (POS - part of speech) oraz Matcher - silnik dopasowywania wzorców.
        </p>

        <p>
        Podczas przetwarzania tekstu spaCy dzieli go na zdania oraz tokeny, każdy token jest
        zwykle słowem lub znakiem interpunkcyjnym. Dla każdego z tokenów spacy określa jaką
        jest częścią mowy, można więc dzięki temu odseparować same rzeczowniki
        występujące w tekście i posortować je w kolejności występowania. Taką pracę wykonuje
        skrypt popular_noun.py
        <label for="3-2" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="3-2" class="margin-toggle"/>
        <span class="sidenote">Do pobrania z serwisu <a href="https://github.com/pjaskulski/zapiski" target="_blank" rel="noopener">github</a>.
        </span>
        .
        </p>

        <p>
        Najpopularniejszy rzeczownik w publikacji Stanisława Bodniaka (<em>Polska a Bałtyk
        za ostatniego Jagiellona</em>) to słowo 'król' występujące w różnych formach ('królów,
        królach, króla, królu, królowi, królem, Króla, Król, król') 435 razy, kolejne
        zaś to 'rok', 'statek', 'straż', 'kapitan', 'komisja', 'okręt', 'żegluga', 'miasto',
        'morze' - co nie jest zaskakujące biorąc pod uwagę tematykę artykułu. Wśród nich
        mamy dwa reprezentujące funkcję: król i kapitan. Przykład fragmentu wyników działania
        skryptu w tabelce poniżej:<br>

        <table>
        <tr><th>Funkcja</th><th>Liczba wystąpień</th><th>Typowe formy</th></tr>
        <tr>
        <td>król</td>
        <td>435</td>
        <td>królów, królach, króla, królu, królowi, królem, Króla, Król, król</td>
        </tr>
        <tr>
        <td>kapitan</td>
        <td>192</td>
        <td>kapitanowie, kapitan, kapitanów, kapitanach, Kapitan, kapitanami, kapitana, kapitanowi, kapitanem, kapitanom</td>
        </tr>
        <tr>
        <td>komisarz</td>
        <td>130</td>
        <td>komisarzom, komisarze, komisarzem, Komisarze, komisarza, komisarzy, komisarz</td>
        </tr>
        </table>
        </p>

        <p>
        Kolejna część pracy należy do człowieka, z otrzymanej listy należy wybrać inne funkcje
        i zawody, znajdziemy tam między innymi funkcje: komisarz, strażnik, kapr, poseł, car, kupiec, żołnierz,
        cesarz, senator, członek, kasztelan, urzędnik, admirał, delegat, mistrz, żeglarz, prezes,
        sekretarz, armator, władca, kanclerz, marynarz, właściciel, dowódca, podkanclerzy,
        namiestnik, książę, ks., starosta, sługa, wojownik, mocodawca, burmistrz. Mając listę
        funkcji można wykorzystując mechanizm Matcher - silnik dopasowywania wzorców biblioteki
        spaCy - wyszukać powiązania funkcji i osób
        <label for="3-3" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="3-3" class="margin-toggle"/>
        <span class="sidenote"><a href="https://spacy.io/usage/rule-based-matching" target="_blank" rel="noopener">Rule-based matching</a>.
        </span>.
        </p>

        <p>
        Wzorzec według którego spaCy dopasowywuje tekst może wyglądać na przykład tak:<br>
        <code>[{"LEMMA": {"IN": lista}}, {"POS":"ADJ", "OP":"?"},{"POS":"PROPN", "OP":"+"}],</code><br>
        gdzie <code>lista</code> jest wyżej wypisaną listą funkcji, <code>LEMMA</code> - oznacza formę podstawową wyrazu,
        <code>"POS":"ADJ"</code> oznacza wyraz będący przymiotnikiem,
        <code>"POS":"PROPN"</code> - oznacza wyraz będący jednostką nazwaną (np. osobą wymienioną z imienia
        i nazwiska), <code>"OP":"?"</code> to operator wskazujący, że poprzedni wzorzec może wystąpić 0 lub 1 raz,
        <code>"OP":"+"</code> zaś jest operatorem oczekującym wystąpienia wzorca co najmniej raz (lub więcej razy).
        </p>

        <p>
        Efektem przetworzenia tekstu badanej publikacji S. Bodniaka przez zestaw podobnych
        reguł (skrypt <code>funkcje_zawody.py</code> - pod tym samym adresem co poprzedni skrypt)
        jest lista wyrażeń zawierających osoby
        i ich funkcje/zawody np.
        <ul>
        <li>"kasztelana chełmińskiego Jerzego Oleskiego"</li>
        <li>"kanclerz Nils Gyllenstierna"</li>
        <li>"admirałem szwedzkim Klausem Flemmingiem"</li>
        <li>"namiestnik Inflant Jan Chodkiewicz"</li>
        <li>"sekretarz królewski Kasper Geschkau"</li>
        <li>"Michała Brunowa kanclerza Gotarda Kettlera"</li>
        </ul>
        </p>

        <p>Możemy stąd uzyskać np. listę występujących w tekście kapitanów i kaprów: Figenow, Gendtrichsen,
        Michał Starosta, Paweł Glasow, Jan Munckenbeck, Kersten Rode, Marcin Śchele, Mateusz Scharping,
        Marek Wilde.
        </p>

        <p>
        Unikalnych znalezisk jest niespełna 90 czyli tylko dla 1/3 osób z indeksu postaci
        stworzonego w poprzedniej zapisce znaleziono ich funkcje lub zawód, ale jest to bardzo
        prosta metoda (pominąłem np. zawody/funkcje występujące mniej niż 10 razy), algortym można zapewne udoskonalić, poza tym te informacje dostajemy
        bez potrzeby ręcznego przeglądania 230 stron publikacji...
        </p>

        </section>
    </article>

    <article>
        <hr>
        <h2 id="2022-10-05">spaCy - jak poprawić wyniki NER czyli Barnim XI to<br> też osoba...
        (<strong>05.10.2022</strong>)</h2>

        <section>
        <p>Rozpoznawanie jednostek nazwanych przeprowadzone na fragmencie publikacji S. Bodniaka
        <label for="4-1" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="4-1" class="margin-toggle"/>
        <span class="sidenote">
        zob. zapiskę z
        <a href="#2022-10-01">1.10.2022</a>.
        </span>
        dało całkiem niezłe rezultaty w zakresie rozpoznawania osób, zdarzały się jednak takie
        nazwy, które mechanizm pominął. Na przykład 'Barnim XI' - książę pomorski. Czy
        można nauczyć spaCy rozpoznawania takich nietypowych imion? Tak, i nie musi się to
        wiązać z trenowaniem własnego modelu. <strong>EntityRuler</strong> jest komponentem potoku
        przetwarzania spaCy służącym do rozpoznawania jednostek nazwanych poprzez reguły
        <label for="4-2" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="4-2" class="margin-toggle"/>
        <span class="sidenote">Dokumentacja komponentu
        <a href="https://spacy.io/usage/rule-based-matching#entityruler" target="_blank" rel="noopener">EntityRuler</a>.
        </span>
        i może zostać użyty do polepszenia wyników modelu statystycznego NER.
        </p>
        <img src="/static/img_zapiski/ner_barnim_1.png" width="625" alt="tekst bez rozpoznania imienia Barnim"/>

        <p>
        Aby użyć komponentu należy dodać go metodą:<br> <code>nlp.add_pipe("entity_ruler")</code> oraz
        zdefiniować wzorce dla encji, które chcemy
        rozpoznawać w badanym tekście, mogą to być proste wzorce, np. pasujący do naszego przypadku:<br><br>
        <code>pattern = [{"label":"persName", "pattern": "Barnim XI"}]</code><br><br>
        Mogą to także być bardziej zaawansowane wzorce oparte na tokenach:<br><br>
        <code>[{"label": "persName", "pattern": [{"LOWER": "barnim"}, {"LOWER": "wielki"}]}]</code><br><br>
        Korzystamy w tym przypadku z istniejącej w modelu <strong>pl_core_news_md</strong> etykiety <em>persName</em> oznaczającej
        imiona i nazwiska osób.
        </p>

        <p>Powtórne przetworzenie analizowanego fragmentu tekstu powoduje już poprawne
        rozpoznanie wyrażenia 'Barnim XI' jako osoby (label = <em>persName</em>)
        </p>
        <img src="/static/img_zapiski/ner_barnim_2.png" width="625" alt="tekst z rozpoznanym imieniem Barnim"/>

        <p>Komponent EntityRuler potrafi jednak znacznie więcej, możemy z jego pomocą poprosić spaCy
        o rozpoznawanie nowej kategorii obiektów np. funkcji pełnionych przez osoby występujące w tekście.
        Zakładając, że interesują nas funkcje: król, hetman, senator oraz chcemy odnaleźć nie
        tylko dokładnie takie wystąpienia słów (np. 'senator') ale i wersję w liczbie mnogiej ('senatorowie') lub w formie
        odmienionej ('senatorowi') należy użyć bardziej zaawansowanego sposobu definiowana wzorca:<br>
        <code>[{"label":"OCCUPATION", "pattern": [{"LEMMA": {"IN":["hetman","senator","marszałek", "król"]}}]}]</code><br><br>
        Nowa etykieta to <code>'OCCUPATION'</code>, <code>'LEMMA'</code> oznacza formę podstawową słowa, np. dla wyrazów 'senator' i 'senatorowie'
        forma podstawowa będzie równa 'senator', zamiast jednej funkcji np. <code>"LEMMA":"hetman"</code> można we wzorcu wskazać
        od razu całą listę funkcji korzystając z zapisu: <code>{"LEMMA": {"IN":["hetman","senator","marszałek", "król"]}</code>.
        Efekt działania tak zdefiowanego wzorca wraz z całym fragmentem kodu widoczny jest dla zrzucie ekranu poniżej,
        a wszystko to bez trenowania własnego modelu.
        <label for="4-3" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="4-3" class="margin-toggle"/>
        <span class="sidenote">Notatnik jupytera z kodem ze zrzutu ekranu jest na
        <a href="https://github.com/pjaskulski/zapiski/blob/main/src/entity_ruler_test.ipynb" target="_blank" rel="noopener">githubie</a>.
        </span>

        Należy oczywiście zdawać sobie sprawę z ograniczeń, dla języka
        polskiego będą występować problemy z lematyzacją, lemma od słowa <em>podkanclerzy</em> to wg spacy 'podkancler',
        ale już lemma od słowa <em>podkanclerzego</em> to 'podkanclerzy'.
        </p>
        <img src="/static/img_zapiski/entity_ruler_funkcje.png" width="625" alt="EntityRuler - funkcje i zawody"/>

        </section>
    </article>

    <article>
        <hr>
        <h2 id="2022-10-06">Artykuły, kursy, filmy - o NLP w badaniach historycznych (<strong>06.10.2022</strong>)</h2>

        <section>
        <p>
        Internet jest pełen świetnych materiałów na temat NLP, uczenia maszynowego,
        programowania w pythonie czy konkretnych narzędzi i bibliotek,
        w tym miejscu chciałbym gromadzić linki przede wszystkim do tych z nich,
        które dotyczą humanistyki cyfrowej, wykorzystania NLP w badaniach historycznych
        lub były przygotowane z myślą o początkujących humanistach cyfrowych.
        </p>

        <p>
        Kurs <em>Natural Language Processing with spaCy & Python - Course for Beginners</em>
        by Dr. William Mattingly na kanale freeCodeCamp.org:
        <a href="https://www.youtube.com/watch?v=dIUTsFT2MeQ" target="_blank" rel="noopener">LINK</a>
        oraz kurs <em>Introduction to spaCy 3</em> w formie podręcznika online:
        <a href="http://spacy.pythonhumanities.com/" target="_blank" rel="noopener">LINK</a>
        </p>

        <p>Więcej świetnych filmów i kusrów tego samego autora dostępnych jest na jego
        własnym kanale YT: "Python Tutorials for Digital Humanities"
        <a href="https://www.youtube.com/pythontutorialsfordigitalhumanities" target="_blank" rel="noopener">LINK</a>
        oraz na jego stronie np.:
        "Introduction To Named Entity Recognition With a Case Study of Holocaust NER"
        <a href="http://ner.pythonhumanities.com/intro.html" target="_blank" rel="noopener">LINK</a>
        </p>

        <p>
        Yvonne Gwerder <em>Named Entity Recognition in Digitized Historical Texts</em>
        (praca magisterska, Universitat Zurich) <a href="https://www.cl.uzh.ch/dam/jcr:cda50f3f-88a3-4e88-a6d9-73deef9df12c/Masterarbeit_YGwerder_FS17.pdf" target="_blank" rel="noopener">PDF</a>
        </p>

        <p>
        Felipe Álvarez de Toledo López-Herrera <em>Automated Tagging of Historical, Non-English
        Sources with Named Entity Recognition (NER): A Resource</em>
        <a href="https://blogs.library.duke.edu/data/2020/08/31/automated-tagging-of-historical-non-english-sources-with-named-entity-recognition-ner-a-resource/" target="_blank" rel="noopener">LINK</a>
        </p>

        <p>
        Vatsala Nundloll, Robert Smail, Carly Stevens, Gordon Blair
        <em>Automating the extraction of information from a historical text and
        building a linked data model for the domain of ecology and conservation
        science</em>
        <a href="https://www.sciencedirect.com/science/article/pii/S2405844022019983?dgcid=rss_sd_all" target="_blank" rel="noopener">LINK</a>
        </p>

        <p>
        Claire Grover, Sharon Givon, Richard Tobin, Julian Ball <em>Named Entity Recognition
        for Digitised Historical Texts</em>
        <a href="https://www.ltg.ed.ac.uk/np/publications/ltg/papers/bopcris-lrec.pdf" target="_blank" rel="noopener">PDF</a>
        </p>

        <p>
        Helena Hubková, <em>Named-entity recognition in Czech historical texts</em>
        <a href="http://www.diva-portal.org/smash/get/diva2:1325355/FULLTEXT01.pdf" target="_blank" rel="noopener">PDF</a>
        </p>

        <p>
        Blog <em>NLP for Historical Texts</em> <a href="https://nlphist.hypotheses.org/" target="_blank" rel="noopener">https://nlphist.hypotheses.org/</a>
        prowadzony przez Michaela Piotrowskiego, autora książki <em>Natural Language
        Processing for Historical Texts</em> (2012) <a href="https://link.springer.com/book/10.1007/978-3-031-02146-6" target="_blank" rel="noopener">LINK</a>
        </p>

        <p>
        Asarsa Kunal <em>Analysis of named entity recognition & entity linking
        in historical text</em> (Masters theses, Northeastern University) <a href="https://repository.library.northeastern.edu/files/neu:cj82nh21q/fulltext.pdf" target="_blank" rel="noopener">PDF</a>
        </p>

        <p>Kimmo Kettunen, Eetu Mäkelä, Teemu Ruokolainen, Juha Kuokkala, Laura Löfberg
        <em>Old Content and Modern Tools – Searching Named Entities in a Finnish OCRed
        Historical Newspaper Collection 1771–1910</em>
        <a href="http://www.digitalhumanities.org/dhq/vol/11/3/000333/000333.html" target="_blank" rel="noopener">LINK</a>
        </p>

        <p>Christian Henriot <em>Rethinking historical research in the age of NLP</em> (blog)
        <a href="https://enepchina.hypotheses.org/3275" target="_blank" rel="noopener">LINK</a>
        </p>

        <p>Magdalena Turska: <em>Informatyk wśród humanistów czyli krótka historia zderzenia
        światów i co z tego wynikło</em> (video, nie dotyczy bezpośrednio NLP, ale wielu ciekawych aspektów
        styku informatyki i humanistyki)
         <a href="https://www.youtube.com/watch?v=qCGaOmXXpvY&t=667s" target="_blank" rel="noopener">LINK</a>
        </p>

        <p>
        E. Oliveira, G. Dias, J. Lima, J.P.C. Pirovani: <em>Using Named Entities for Recognizing Family Relationships</em>
        <a href="https://sol.sbc.org.br/index.php/kdmile/article/view/17457">LINK</a>
        </p>

        <p>
        Sam Fields, Camille Lyans Cole, Catherine Oei, Annie T Chen: <em>Using named entity recognition
        and network analysis to distinguish personal networks from the social milieu in
        nineteenth-century Ottoman–Iraqi personal diaries</em>
        <a href="https://academic.oup.com/dsh/advance-article/doi/10.1093/llc/fqac047/6658426">LINK</a>
        </p>

        <p>Audrey Holmes: <em>Named Entity Resolution for Historical Texts</em> (praca magisterska, University of Washington, 2019)
        <a href="https://digital.lib.washington.edu/researchworks/bitstream/handle/1773/44844/Holmes_washington_0250O_20778.pdf">LINK</a>
        </p>

        <p>Nicolas Eymael Da Silva: <em>Extraction of entities and relations in Portuguese from the Second
        HAREM Golden Collection</em> (praca licencjacka, Universidade Federal Do Rio Grande Do Sul)
        <a href="https://www.lume.ufrgs.br/bitstream/handle/10183/224833/001129035.pdf?sequence=1">PDF</a>
        </p>

        <p>N. Abadie, E. Carlinet, J. Chazalon, and B. Dumenieu:
        <em>A Benchmark of Named Entity Recognition Approaches in Historical Documents
        Application to 19th Century French Directories</em>
        <a href="https://hal.archives-ouvertes.fr/hal-03698609/document">PDF</a>
        </p>

        <p>Marcella Tambuscio, Tara Lee Andrews:
        <em>Geolocation and Named Entity Recognition in Ancient Texts: A Case Study
        about Ghewond’s Armenian History</em>
        <a href="http://ceur-ws.org/Vol-2989/short_paper28.pdf">PDF</a>
        </p>

        <p>Leonardo Zilio, Maria Jos´e Bocorny Finatto, and Renata Vieira:
        <em>Named Entity Recognition Applied to Portuguese Texts from the XVIII Century</em>
        <a href="https://dspace.uevora.pt/rdpc/bitstream/10174/32165/1/paper10-NER.pdf">PDF</a>
        </p>

        <p>Alistair Plum, Tharindu Ranasinghe, Spencer Jones, Constantin Orasan, Ruslan Mitkov:
        <em>Biographical: A Semi-Supervised Relation Extraction Dataset</em>
        <a href="https://arxiv.org/pdf/2205.00806.pdf">PDF</a>
        </p>

        <p>Nitisha Jain, Alejandro Sierra-Múnera, Julius Streit, Simon Thormeyer, Philipp Schmidt, Maria Lomaeva and Ralf Krestel:
        <em>Generating Domain-Specific Knowledge Graphs: Challenges with Open Information Extraction</em>
        <a href="http://ceur-ws.org/Vol-3184/TEXT2KG_Paper_4.pdf">PDF</a>
        </p>

        <p>Fabio Chiusano: <em>Building a Knowledge Base from Texts: a Full Practical Example</em> (blog)
        <a href="https://medium.com/nlplanet/building-a-knowledge-base-from-texts-a-full-practical-example-8dbbffb912fa">LINK</a>
        </p>

        <p>Tomaz Bratanic: <em>Extract knowledge from text: End-to-end information extraction pipeline with spaCy and Neo4j</em>
        (blog)
        <a href="https://towardsdatascience.com/extract-knowledge-from-text-end-to-end-information-extraction-pipeline-with-spacy-and-neo4j-502b2b1e0754">LINK</a>
        </p>

        </section>
    </article>

    <article>
        <hr>
        <h2 id="2022-10-07">Anotacja tekstów w Doccano i trenowanie własnego<br> modelu (<strong>07.10.2022</strong>)</h2>

        <section>
        <p>
        Wykorzystanie reguł do wyszukiwania encji nazwanych poprzez komponent EntityRuler
        biblioteki spaCy jest całkiem skutecznym sposobem poprawienia wyników NER. Możemy zastosować ten mechanizm
        jeżeli w używanym modelu brakuje np. jednej etykiety i łatwo jest uzupełnić ten brak
        za pomocą zestawu słów kluczowych, tworząc odpowiednie reguły. W trakcie prac nad tekstami spotkamy się
        jednak na pewno z bardziej skomplikowanymi przypadkami, a wówczas niezbędne stanie
        się wytrenowanie własnego modelu, dopasowanego do specyfiki naszych problemów. Przed
        procesem trenowania niezbędne jest jednak przejście przez trzy wstępne etapy: 1) należy zgromadzić zestaw
        danych (tekstów) do uczenia, 2) przygotować anotacje tych tekstów, 3) zdecydować, czy
        będziemy trenować własny model od zera, czy też dotrenowywać już istniejący. Krok
        drugi wiąże się też z wyborem oprogramowania do anotacji oraz przygotowaniem jej
        wyników do formy akceptowalnej przez spaCy.
        </p>

        <p>
        Celem testu będzie przygotowanie danych, a następnie wytrenowanie modelu rozpoznającego
        zawody/funkcje (dla uproszczenia będzie to mały model trenowany 'od zera'). Próbką tekstów
        do anotacji w naszym teście będzie zbiór fragmentów artykułu S. Bodniaka
        <label for="6-1" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="6-1" class="margin-toggle"/>
        <span class="sidenote">Zob. zapiskę z
        <a href="#2022-10-01">1.10.2022</a>.
        </span>,
        pochodzących z różnych części tej publikacji, lecz zawsze zawierających określenia
        nazw zawodów lub funkcji. Zbiór liczy w sumie 105 zdań, czyli parę procent
        całego tekstu, zapewne dla osiągnięcia naprawdę dobrych wyników powinien
        być wyraźnie większy (wspomina się o minimum 100 przykładach dla każdej uczonej encji).
        Najlepszą aplikacją do anotacji współpracującą z spaCy jest z pewnoścą Prodigy
        <label for="6-2" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="6-2" class="margin-toggle"/>
        <span class="sidenote">
        <a href="https://prodi.gy/" target="_blank" rel="noopener">Strona</a> aplikacji prodigy.
        </span>
        ,
        narzędzie przygotowane przez autorów spaCy - firmę Explosion. Jest to jednak
        oprogramowanie komercyjne i, choć w realnej sytuacji z pewnością warto w nie
        zainwestować (ceny zaczynają się od 390 USD za licencję), to w naszym teście
        użyjemy narzędzia dostępnego bezpłatnie. Istnieje ich co najmniej kilka, np.
        <a href="https://tecoholic.github.io/ner-annotator/" target="_blank" rel="noopener">ner-annotator</a>,
        <a href="https://github.com/d5555/TagEditor" target="_blank" rel="noopener">TagEditor</a> (aplikacja desktopowa
        dla systemu Windows),
        <a href="https://brat.nlplab.org/" target="_blank" rel="noopener">brat</a>,
        <a href="https://inception-project.github.io/" target="_blank" rel="noopener">INCEpTION</a> czy
        <a href="https://github.com/doccano/doccano" target="_blank" rel="noopener">doccano</a>. Każde z nich na pewno
        dobrze się sprawdzi przy wprowadzaniu etykiet, jednak najlepsze wrażenie zrobiło
        na mnie <strong>doccano</strong>, aplikacja webowa, którą można w prosty sposób zainstalować
        lokalnie korzystając z dockera. Opis instalacji znajduje się na stronie projektu,
        polecam ścieżkę z wykorzystaniem docker-compose.
        <label for="6-3" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="6-3" class="margin-toggle"/>
        <span class="sidenote">doccano - instalacja poprzez
        <a href="https://github.com/doccano/doccano#docker-compose" target="_blank" rel="noopener">docker-compose</a>.
        </span>
        </p>

        <p>
        Praca w aplikacji doccano zaczyna się od zalogowania na nasze konto (podczas
        instalacji zakładane jest konto użytkownika będącego administratorem, wraz z hasłem,
        później można utworzyć dodatkowe konta nadając im role np. anotatora,
        czy osoby akceptującej anotacje) i utworzenia projektu. Zakładając nowy projekt
        należy wskazać typ projektu (rodzaj zadania), w naszym przypadku chcemy 
        rozpoznawać jednostki nazwane (NER), należy więc wybrać <em>sequence labeling</em>.
        </p>
        <img src="/static/img_zapiski/doccano_lista_projektow.png" width="600" alt="doccano lista projektów"/>

        <p>
        Do utworzonego projektu można zaimportować dane do anotacji (menu Dataset),
        <strong>doccano</strong>
        akceptuje różne pliki tekstowe (lub zestawy plików), zaimportowane dane będą widoczne
        w formie tabeli.
        </p>
        <img src="/static/img_zapiski/doccano_lista.png" width="600" alt="doccano lista dokumentów"/>

        <p>
        Przed rozpoczęciem anotowania niezbędne jest jeszcze utworzenie etykiet, którymi
        będzie oznaczany tekst. Biorąc pod uwagę cel testu jedyną używaną etykietą
        będzie <em>occupation</em> na oznaczenie zawodu lub funkcji pełnionej przez
        postaci występujące w tekście.
        </p>
        <img src="/static/img_zapiski/doccano_tworzenie_etykiet.png" width="600" alt="doccano tworzenie etykiet"/>

        <p>Sam proces anotacji w doccano jest bardzo zbliżony do pracy w innych aplikacjach
        tego typu. Zaznaczamy słowo lub wyrażenie, program wyświetla wówczas podręczne menu,
        z którego można wybrać właściwą etykietę. Każda z etykiet ma swój kolor (definiowany
        przez użytkownika na etapie tworzenia etykiet) i takim kolorem <strong>doccano</strong>
        podkreśla oznaczony fragment tekstu (wyświetla również nazwę etykiety).
        </p>
        <img src="/static/img_zapiski/doccano_w_trakcie_anotacji.png" width="600" alt="doccano w trakcie anotacji"/>

        <p>
        Po zakończonym procesie anotacji można otagowany zbiór danych wyeksportować do
        formatu JSONL, w którym znajduje się anotowany tekst oraz etykiety i ich pozycje
        w tekście. Niestety, nie jest to gotowy plik, który możemy użyć do trenowania modelu
        z użyciem spaCy. W obecnej wersji (3.4 w momencie pisania zapiski) spaCy oczekuje
        pliku binarnego z przyjętym zwyczajowo rozszerzeniem *.spacy (format DocBin). Na
        szczęście konwersja pliku *.jsonl na *.spacy nie jest skomplikowana, można do tego
        celu użyć przykładowego skryptu <code>doccano2spacy.py</code>, który wywoływany
        jest z nazwą pliku jsonl jako jedynym parametrem.
        <label for="6-4" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="6-4" class="margin-toggle"/>
        <span class="sidenote">Skrypt można pobrać z repozytorium
        <a href="https://github.com/pjaskulski/zapiski/blob/main/src/doccano2spacy.py" target="_blank" rel="noopener">github</a>.
        </span>
        </p>
        <img src="/static/img_zapiski/doccano_fragment_pliku_jsonl.png" width="600" alt="doccano fragment pliku jsonl"/>

        <p>
        Mając zbiór uczący, można przystąpić do trenowania nowego modelu. Proces trenowania
        wywoływany jest z linii komend poleceniem <code>spacy train</code>, należy również 
        podać nazwę pliku
        konfiguracyjnego (np. config.cfg) z parametrami uczenia, spaCy ułatwi jego
        utworzenie z domyślnymi parametrami. Procedura opisana jest na stronie
        <a href="https://spacy.io/usage/training#quickstart" target="_blank" rel="noopener">spacy.io</a> (tak, opis jest
        długi - to jest skomplikowane!). Należy też podać ścieżkę do zbioru uczącego
        i walidacyjnego (aby zachować reguły gry należałoby także przygotować zbiór walidujący,
        tu dla uproszczenia skorzystam z kopii zbioru uczącego). Parametr <code>--output</code>
        polecenia wskazuje, gdzie będą zapisane wytrenowane modele (spacy zapisuje ostatni
        i najlepszy z modeli).<br>
        <code>python -m spacy train config.cfg --output ./output --paths.train ./bodniak.spacy --paths.dev ./bodniak.spacy</code><br>
        Podczas działania (dla większych zbiorów czas wykonywania obliczeń może być spory, w tym przypadku
        będzie to kilkadziesiąt sekund) wyświetlany jest stan kolejnych iteracji uczenia
        (zob. zrzut ekranu poniżej).
        </p>
        <img src="/static/img_zapiski/proces_trenowania_modelu.png" width="600" alt="proces trenowania modelu"/>

        <p>
        Po zakończeniu trenowania można przystąpić do weryfikacji działania modelu.
        Wykorzystamy do tego środowisko <strong>jupyter</strong> i mechanizm wizualizacji
        <strong>displacy</strong>. Wczytanie nowego modelu w spaCy odbywa się podobnie
        jak w przypadku modelu standardowego z tym, że zamiast nazwy należy, podać ścieżkę do katalogu
        z zapisanym modelem: <code>nlp = spacy.load('../output/model-best/')</code>.<br> Wynik
        działania na spreparowanym zdaniu, w którym występują nazwy funkcji/zawodów widać
        na załączonym niżej zrzucie ekranu z fragmentem notebooka aplikacji jupyter.
        <label for="6-5" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="6-5" class="margin-toggle"/>
        <span class="sidenote">Notebook test_ner_model.ipynb do pobrania z
        <a href="https://github.com/pjaskulski/zapiski/tree/main/src" target="_blank" rel="noopener">repozytorium</a>.
        </span>

        Można zauważyć poprawne rozpoznanie słów 'król', 'kapitan', 'kanclerzem', jednak
        wyrażenie 'podstarościm' pozostało niezauważone, najwyraźniej model nie zdołał się
        go nauczyć, w materiale uczącym występowało ono faktycznie niezbyt często.
        </p>
        <img src="/static/img_zapiski/test_modelu_ner.png" width="650" alt="test modelu ner"/>

        <p>
        I jeszcze dwa przydatne linki: do dokumentacji aplikacji doccano
        <a href="https://doccano.github.io/doccano/" target="_blank" rel="noopener">LINK</a>,
        oraz do artykułu, który dużo obszerniej opisuje tę aplikację
        <a href="https://towardsdatascience.com/doccano-a-tool-to-annotate-text-data-to-train-custom-nlp-models-f4e34ad139c3" target="_blank" rel="noopener">LINK</a>.
        </p>
        </section>

    </article>

    <article>
        <hr>
        <h2 id="2022-10-10">Skrypty w poszukiwaniu wydarzeń (<strong>10.10.2022</strong>) </h2>

        <section>
        <p>
        Strona xvi-wiek.pl to przede wszystkim kalendarium ciekawych wydarzeń z XVI wieku,
        niektórych bardziej znanych, innych zaś zupełnie egzotycznych. Skąd jednak te informacje
        pochodzą? Spora część (także) z wikipedii, ale większość z drukowanych książek i artykułów
        a sposób ich wyszukiwania nie polegał najczęściej na spędzaniu wieczorów z herbatą i książką
        (choć to przyjemne zajęcie), lecz miał coś wspólnego z cyfrową humanistyką. Wiele współcześnie
        wydawanych publikacji jest już dostępnych od razu w formie elektronicznej, coraz więcej
        starszych jest digitalizowanych i udostępnianych w formie plików pdf (już rzadziej jako DjVu).
        Jeżeli zaś publikacja ma postać cyfrowego pliku (z tekstem), to pozwala na przetwarzanie
        jej przez narzędzia informatyczne.
        </p>

        <p>
        Wpisy w kalendarium są faktami i wydarzeniami związanymi z konkretną datą dzienną. W przypadku
        xvi-wieku.pl przyjęte też zostały określone ramy czasowe: od 1490 do 1586 roku włącznie. Daty
        dzienne mogą być zapisane w różnej formie, bardziej nowoczesnej np. <strong>12.08.1560</strong>,
        trochę bardziej tradycyjnej np. <strong>12 VIII 1560</strong>, lub z wprost podaną nazwą miesiąca:
        <strong>12 sierpnia 1560 r.</strong> Mogą to też być fragmenty dat typu "12 stycznia" gdyż z
        kontekstu wynika o jaki rok chodzi.
        </p>

        <p>
        Posiadając zgromadzoną kolekcję plików pdf (pochodzących główne z repozytoriów
        cyfrowych RCIN, bazhum.muzhp.pl  i innych źródeł o otwartym dostępie) można wyszukać
        w nich daty dzienne z wymaganego okresu, nie chodzi oczywiście o ręczne przeszukiwanie
        w przegladarce PDF-ów każdego oczekiwanego wariantu daty, można skorzystać z tzw. wyrażeń regularnych
        <label for="7-1" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="7-1" class="margin-toggle"/>
        <span class="sidenote">Definicja wyrażeń
        <a href="https://pl.wikipedia.org/wiki/Wyra%C5%BCenie_regularne" target="_blank" rel="noopener">regularnych</a>
        w Wikipedii oraz ładne wprowadzenie do <a href="https://miroslawmamczur.pl/wyrazenia-regularne-czym-sa-i-jak-pisac-wlasne-regexy/" target="_blank" rel="noopener">tematu</a>.
        </span>
        i jednego z narzędzi, które na przeszukiwanie plików pdf w taki sposób pozwalają. Dla osób 
        pracujących w środowisku linuksowym pierwszym wyborem będzie zwykle narzędzie konsolowe np.
        <a href="https://pdfgrep.org/" target="_blank" rel="noopener">pdfgrep</a> (w systemie Windows na pewno istnieją równie
        dobre i wygodniejsze odpowiedniki np. <a href="http://dngrep.github.io/" target="_blank" rel="noopener">dnGrep</a>).
        Program pozwala na przetworzenie bieżącego katalogu, lub całego drzewa katalogów zawierającego
        pliki pdf i przeszukanie tekstowej zawartości pdf-ów wg zadanego wzorca.
        </p>

        <p>
        Zakładając, że wzorzec powinien uwzględniać dzień - 1 lub 2 cyfry, miesiąc w formie liczby
        rzymskiej lub nazwy oraz rok czerocyfrowy, parametry wywołania programu pdfgrep mogłyby
        wygladać np. tak:<br><br>
        <code>pdfgrep -rnPH "\s+\d{1,2}\s+([a-z]{3,}|[XVI]+)\s+(15|149)" *.pdf</code><br><br>
        gdzie <code>-rnPH</code> to parametry programu decydujące o wyszukiwaniu rekurencyjnym plików
        (czyli program będzie szukał w podkatalogach i podkatalogach katalogów), o wypisywnaiu w wynikach
        numerów stron, o użyciu wyrażeń regularnych i podawaniu w wyniku nazw plików pdf, pozostała część
        polecenia to wyrażenie zwracające daty oraz ścieżka do plików (z maską). Wyrażenie regularne ma trzy
        części <code>\d{1,2}</code> poszukuje liczby o długości od jednej do 2 cyfr, fragment
        <code>([a-z]{3,}|[XVI]+)</code> szuka rzymskiego numeru miesiąca lub nazwy miesiąca, ostatnia część
        <code>(15|149)</code> odpowiada za rok, poszukiwana jest liczba 15 lub 149 będąca początkiem roku
        (co jest drobnym problemem bowiem oznacza, że wpadną w wynikach także lata 1587-99).
        </p>

        <p>
        Uruchomienie powyższego polecenia w katalogu z publikacją S. Bodniaka zwraca 24 wyniki
        (zob. zrzut ekranu poniżej). Program wypisuje nazwę pliku pdf w którym znaleziono wzorzec, numer strony (zielona liczba), oraz krótki
        fragment tekstu ze znalezionym wzorcem, gdzie wzorzec jest wyróżniony czerwonym kolorem. Pozostaje
        zweryfikować datę na podanej stronie w pliku pdf, być może jest to data wydarzenia, które
        może trafić do kalendarium.
        </p>
        <img src="/static/img_zapiski/pdfgrep_bodniak.png" width="650" alt="pdfgrep wyniki przeszukiwania.png"/>

        <p>
        Co jednak, gdyby wyszukiwanie miało bardziej ambitny cel, na przykład znalezienie dat dziennych,
        ale występujących w tym samym zdaniu co postać historyczna i może jednocześnie z jakąś nazwą geograficzną?
        Takie zadanie może być trudne do wykonania za pomocą samych wyrażeń regularnych, ale będzie
        świetnym ćwiczeniem z wykorzystaniem biblioteki spaCy. Dla uproszczenia testowi poddany
        zostanie fragment tekstu, w którym tylko jedno ze zdań pasuje do określonych założeń.
        </p>
        <img src="/static/img_zapiski/date_text_example.png" width="650" alt="przykładowy fragment tekstu"/>

        <p>
        Przetworzenie tekstu przy pomocy biblioteki pozwoli rozpoznać nazwy własne  i wyrażenia temporalne.
        </p>
        <img src="/static/img_zapiski/date_analiza_ner.png" width="650" alt="daty analiza ner"/>

        <p>
        spaCy zadba także o podział tekstu na zdania. Aby wyszukać te z nich, które spełniają
        określone wyżej kryteria wystarczy zbadać dla każdego zdania, czy występuje w nim osoba,
        nazwa geograficzna i data. Każda rozpoznana encja posiada przypisaną etykietę, te mogą
        się różnić zależnie od użytego modelu (w tym przykładzie wczytany został największy
        standardowy model dostępny dla języka polskiego - <strong>pl_core_news_lg</strong>).
        Encje osób oznaczane są etykietą 'persName', encje miejsc jako 'placeName',
        a wyrażenia temporalne jako 'date'. Prosty kod filtrujący wraz z wynikiem widoczny jest
        na zrzucie ekranu poniżej. Program dodatkowo sprawdza czy znaleziona data jest datą dzienna
        z lat 1490-1599, oraz czy encja 'placeName' jest nazwą zaczynającą się z dużej litery (aby
        pominąć znaleziska typu 'król <em>szwedzki</em>').
        </p>
        <img src="/static/img_zapiski/date_filter_code.png" width="650" alt="kod filtrujący i wynik działania"/>

        <p>Wydrukowane zdanie jest jedynym z przetwarzanego fragmentu tekstu zawierającym
        osobę (lub osoby), miejscowość lub inną nazwę geograficzną oraz datę - z dokładnością 
        do konkretnego dnia. Można teraz tę samą metodę zastosować
        do całej treści publikacji Stansława Bodniaka, przy czym ponieważ spaCy pracuje raczej na tekstach
        (a nie plikach binarnych) użyty zostanie nie plik pdf, ale plik tekstowy z zawartością pdf-a, przygotowany w jednej
        z poprzednich zapisek.
        <label for="7-2" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="7-2" class="margin-toggle"/>
        <span class="sidenote">
        zob. zapiskę z
        <a href="#2022-10-01">1.10.2022</a>.
        </span>
        Kod przygotowany w notebooku można zapisać w formie funkcji, która będzie wywoływana
        dla każdej strony przetwarzanego tekstu, wynik w postaci numeru strony i znalezionego zdania
        spełniającego przyjęte warunki będzie wypisywany na ekran konsoli.
        <label for="7-3" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="7-3" class="margin-toggle"/>
        <span class="sidenote">Notebook wyszukiwanie_dat.ipynb i skrypt events_finder.py do pobrania z
        <a href="https://github.com/pjaskulski/zapiski" target="_blank" rel="noopener">repozytorium</a>.
        </span>
        Efekt działania skryptu to około 20 znalezisk, które mogą być potencjalnymi wydarzeniami, wartymi opisania w kalendarium (fragment wyniku
        na zrzucie ekranu zamieszczonym poniżej).
        </p>
        <img src="/static/img_zapiski/event_finder_results.png" width="650" alt="wynik działania skryptu event_finder.py"/>

        </section>
    </article>

    <article>
        <hr>
        <h2 id="2022-10-17">spaCy, NEL - łączenie rozpoznanych encji z wikidata.org (<strong>17.10.2022</strong>) </h2>

        <section>
        <p>
        Rozpoznanie w tekście nazw własnych może być wstępem do następnego kroku - połączenia
        znalezionych encji np. osób z bazą wiedzy, uzyskujemy wówczas identyfikację danej osoby,
        pojawiający się w analizowanej publikacji Fryderyk II staje się tożsamy z duńskim królem
        żyjącym w XVI wieku, który w bazie wiedzy np. wikidata.org posiada jednoznaczny identyfikator
        <code>Q154041</code>. Proces łączenia encji z identyfikatorami z baz wiedzy określany jest często jako
        <em>named-entity linking</em> (czyli NEL)
        <label for="8-1" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="8-1" class="margin-toggle"/>
        <span class="sidenote">
        <a href="https://en.wikipedia.org/wiki/Entity_linking" target="_blank" rel="noopener">Entity linking</a>.
        </span>
        lub <em>named-entity disambiguation</em> i nie
        jest tak prostym zadaniem jak się pozornie wydaje. Fryderyków II czy miejscowości o nazwie
        'Warszawa' może w bazie wiedzy istnieć wiele, nazwa miejscowości w publikacji może być 
        zniekształcona (może być jednym z używanych wariantów nazwy), może być nazwą używaną w XVI
        wieku, lub nazwą polską używaną przed 1939 rokiem dla miejscowości dziś leżącej poza Polską.
        Zygmunt II August może w analizowanym tekście występować także jako <em>Zygmunt August</em>,
        lub po prostu jako <em>król Zygmunt</em>.Trudność polega właśnie na automatycznym
        dopasowaniu właściwego elementu z bazy wiedzy do encji z publikacji, z uwzględnieniem
        drobnych różnic w pisowni oraz kontekstu.
        </p>

        <p>
        Jednym z możliwych rozwiązań problemu łączenia encji z tekstu z bazami wiedzy jest obecny
        w bibliotece spaCy komponent EntityLinker
        <label for="8-2" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="8-2" class="margin-toggle"/>
        <span class="sidenote">Dokumentacja na stronie
        <a href="https://spacy.io/api/entitylinker" target="_blank" rel="noopener">spaCy</a>.
        </span>
        . Pozwala on na wytrenowanie własnego modelu, który na podstawie przygotowanej bazy
        wiedzy, rozpoznanej encji nazwanej i kontekstu w której wystąpiła będzie potrafił
        wskazywać właściwy dla niej identyfikator.
        </p>

        <p>
        W ramach testu
        <label for="8-3" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="8-3" class="margin-toggle"/>
        <span class="sidenote">Test przygotowany na bazie tutorialu z
        <a href="https://github.com/explosion/projects/tree/v3/tutorials/nel_emerson" target="_blank" rel="noopener">repozytorium przykładów spaCy</a>
        </span>
        opartego jak zwykle na tekście publikacji S. Bodniaka rozpoznawane i łączone z bazą 
        wiedzy będą dwie postacie: króla Zygmunta II Augusta i cara Ivana IV Groźnego, źródłem
        bazy wiedzy będzie zaś wikidata.org. Identyfikatory z Wikidata (QID) dla obu postaci
        to odpowiednio Q54058 i Q7996. W przypadku analizowanej publikacji nie ma właściwie
        jakiejś wieloznaczności w encjach osób, Zygmunt August będzie zawsze tym Zygmuntem,
        królem Polski, Iwan będzie carem Rosji. Większym wyzwaniem byłoby gdyby w tekście
        występował np. Mateusz Scharping jako kaper i inna postać np. Ernest Scharping będącą
        np. niemieckim dyplomatą, wówczas dla encji będących samym nazwiskiem łączenie
        z identyfikatorami z wikidata.org musiałoby następować na podstawie kontekstu. Tu
        jednak nie ma takiej sytuacji, do bazy można natomiast dodać dodatkowe aliasy
        naszych postaci, również pewnie identyfikowalne z elementami wikidata np. 'Zygmunt August',
        'Iwan IV', 'Iwan Groźny'.
        </p>
        <img src="/static/img_zapiski/knowledge_base_nel.png" width="650" alt="tworzenie bazy wiedzy"/>

        <p>
        Oprócz obiektu KnowledgeBase (naszej bazy wiedzy) potrzebne są również próbki tekstów do
        trenowania modelu - przygotowane dane treningowe na podstawie zdań z artykułu
        Stanisława Bodniaka zawierających wzmianki na temat analizowanych postaci. Dane
        treningowe zawierają w przypadku tego testu około 30 zdań (zapewne powinno być
        trochę więcej). Tak jak w przypadku anotacji dla trenowania modelu NER tu również
        anotacja została przeprowadzona w programie <strong>doccano</strong>, a dane wyjściowe z doccano
        wymagały przetworzenia na format oczekiwany przez spaCy (tu z pliku *.jsonl dane
        importowane są bezpośrednio do skryptu w pythonie). Ponieważ jednak doccano nie
        wspiera bezpośrednio anotacji pod kątem NEL, a przynajmniej nie widzę takiej opcji,
        zastosowane zostało małe obejście problemu - ze względu na małą liczbę anotowanych
        postaci w roli etykiet użyto identyfiktorów QID z wikidata.
        </p>
        <img src="/static/img_zapiski/doccano_nel.png" width="650" alt="anotacja doccano nel"/>

        <p>
        Mając przygotowany do trenowania dataset należy go podzielić na cześć treningową i
        część walidującą, zwykle 20% zbioru wystarcza do walidacji. Proces trenowania dla
        tak małego zestawu danych nie powinien trwać dłużej niż 30 sekund.
        </p>
        <img src="/static/img_zapiski/proces_trenowania_nel.png" width="650" alt="proces trenowania nel"/>

        <p>
        Wynik zaś można zweryfikować na testowym fragmencie artykułu, który nie był częścią
        zbioru uczącego. Jak widać na zrzucie poniżej, zarówno Zygmunt August jak i Iwan Groźny
        zostali rozpoznani i zidentyfikowani poprawnie, jednak po przetworzeniu całej publikacji
        można zauważyć, że wyniki dla cara są jednak słabsze, model nie zawsze przypisuje
        identyfikator z wikidata do rozpoznanej encji tej postaci. Problemem oprócz może zbyt małej próbki
        treningowej może być język polski, trudniejszy w przetwarzaniu od angielskiego choćby
        ze względu na fleksję.
        <label for="8-4" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="8-4" class="margin-toggle"/>
        <span class="sidenote">Notebook NEL_test.ipynb do pobrania z
        <a href="https://github.com/pjaskulski/zapiski" target="_blank" rel="noopener">repozytorium</a>.
        </span>
        <img src="/static/img_zapiski/spacy_nel_wyniki.png" width="650" alt="wyniki analizy nel"/>
        </p>

        <p>Sposób łączenia i identyfikacji z bazami wiedzy dostępny w spaCy jest jednak tylko
        jedną z wielu możliwości
        <label for="8-5" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="8-5" class="margin-toggle"/>
        <span class="sidenote">Zob. także narzędzie
        <a href="https://wiki.clarin-pl.eu/pl/nlpws/services/Elinker" target="_blank" rel="noopener">elinker</a>
         przygotowane przez konsorcjum Clarin-PL.
        </span>
        . Przed dołączeniem do biblioteki komponentu EntityLinker powstawały rozwiązania
        przygotowane przez użytkowników spaCy np. pakiet spacy-entity-linker
        <label for="8-6" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="8-6" class="margin-toggle"/>
        <span class="sidenote">Zobacz
        <a href="https://pypi.org/project/spacy-entity-linker/" target="_blank" rel="noopener">opis</a>.
        </span>.
        Pomysł zakładał przygotowanie lokalnej bazy wiedzy (w formacie sqlite) opartej
        na wikidata i dopasowywanie pojęć w przetwarzanym tekście do etykiet i aliasów elementów
        w wikidata. Takie podejście nie wymagało trenowania modelu, nie uwzględniało jednak
        kontekstu w którym występowało badane wyrażenie. Przygotowana baza wiedzy oparta jest
        na angielskiej instancji wikidata a jej zmiana na inną, jak opisano w dokumentacji
        pakietu, nie jest niestety prosta.
        </p>

        <p>
        Można za to przetestować zbliżone, ale mniej wydajne rozwiązanie
        w postaci bezpośredniego odpytywania przez API serwisu wikidata.org.
        Rozpoznawaniu i łączeniu będą poddawane
        tylko postacie (encje z etykietą <em>persName</em> ropoznane przez spaCy), tylko w polskiej wersji językowej
        wikidata, a otrzymane wyniki będą dodatkowo weryfikowane poprzez sprawdzenie czy znaleziony
        element posiada właściwość <em>instance of</em> = 'human' oraz czy właściwość
        <em>date of death (P570)</em> z wikidata wskazuje na okres historyczny tożsamy z chronologią 
        tematyki artykułu. Funkcja <code>wikilinker</code>, której kod jest dostępny w
        <a href="https://github.com/pjaskulski/zapiski/blob/main/src/NEL_test.ipynb">notebooku</a>
        wspomianym wyżej, ma trzy obowiązkowe argumenty: string z nazwą szukanej postaci,
        najwcześniejsza data śmierci postaci, najpóźniejsza data śmierci postaci (oraz opcjonalny
        czwarty argument z liczbą rozważanych elementów wyszukanych w wikidata - zwykle
        wyszukiwanie zwraca więcej niż 1 pozycję, szczególnie dla mniej precyzyjnych tekstów
        np. 'Albrecht' ta liczba może być spora, domyślnie przyjęto 10). Znając tematykę
        artykułu i okres historyczny jakiego dotyczy możemy w wyszukiwaniu uwzlgędnić zakres
        lat 1550-1625, co znacznie poprawi precyzję wyszukiwania. Fragment kodu i efekt
        wyszukiwania widoczny jest na zrzucie poniżej.
        </p>
        <img src="/static/img_zapiski/wikilinker_example.png" width="650" alt="wikilinker wyszukiwanie"/>

        <p>
        W 9 przypadkach na 10, funkcja <code>wikilinker</code> znalazła prawidłowy identyfikator postaci w
        wikidata.org, udało się to nawet dla imienia 'Albrechta', zapewne trochę przypadkiem dla
        podanego zakresu lat (zakresu dla daty śmierci postaci) i imienia pierwszy znaleziony
        element to właśnie Albrecht Hohenzollern. Jedyna nieznaleziona postać to Szymon
        Maricjus-Czystochlebski - prawdopodobnie jest to Q9352684 (Szymon Marycjusz, alias:
        Szymon Maricjusz z Pilzna)
        <label for="8-7" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="8-7" class="margin-toggle"/>
        <span class="sidenote">Zobacz
        <a href="https://www.wikidata.org/wiki/Q9352684" target="_blank" rel="noopener">w wikidata</a>.
        </span>
        , ale różnica między encją NER w tekście a etykietą w
        wikidata.org jest zbyt duża by identyfikator mógł zostać znaleziony. Można też podejrzewać,
        że nawet gdyby etykieta lub alias tego elementu w wikidata miały formę taką jak występuje w tekście,
        to ze względu na fleksję wyszukiwanie również by się nie powiodło
        <label for="8-8" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="8-8" class="margin-toggle"/>
        <span class="sidenote">Przed wyszukiwaniem w wikidata.org skrypt dokonuje lematyzacji
        encji (przekształca imię i nazwisko postaci do formy podstawowej) jednak ta funkcjonalność
        w używanym modelu spaCy nie jest doskonała, dla rzadszych imion, nazwisk będzie raczej niepoprawna.
        </span>.
        </p>

        <p>
        Oczywiście proste funkcje tego typu nie zastąpią prawdziwych mechanizmów NEL, jednak gdy możemy
        z góry sprecyzować zakres łączenia (tylko osoby, tylko polska wikidata, tylko określony zakres
        chronologiczny) rezulat bywa całkiem znośny.
        </p>

        <p>Eksperymenty z automatyczną anotacją i łączeniem z bazą wiedzy można przeprowadzić
        bez programowania, np. dzięki stronie
        <a href="https://wikifier.org/">wikifier.org</a>, która analizuje wklejony przez użytkownika
        tekst, łącząc znalezione encje/pojęcia z hasłami wikipedii.
        <label for="8-9" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="8-9" class="margin-toggle"/>
        <span class="sidenote">Wikifier obsługuje 100 języków, w tym polski, jego sposób działania opisany jest
        w artykule <a href="https://ailab.ijs.si/dunja/SiKDD2017/Papers/Brank_Wikifier.pdf" target="_blank" rel="noopener">
        Annotating Documents With Relevant Wikipedia Concepts</a>.
        </span>
        Na zrzucie ekranu poniżej widoczny jest
        efekt przetworzenia przez Wikifier fragmentu publikacji S. Bodniaka.
        </p>
        <img src="/static/img_zapiski/wikifier_org.png" width="650" alt="strona wikifier.org"/>

        </section>
    </article>

    <article>
        <hr>
        <h2 id="2022-10-22">spaCy i automatyczne tagowanie encji w TEI Publisherze (<strong>22.10.2022</strong>) </h2>

        <section>
        <p>
        Czy trzeba programować by zajmować się przetwarzaniem języka naturalnego? Prosta odpowiedź
        brzmi: nie, można przecież wynająć programistę lub specjalistę data science. Ale ponieważ
        czas programistów kosztuje, a komunikacja między humanistami a specjalistami technicznymi
        bywa skądinąd wyzwaniem i często rzutuje na jakość wyników, można skorzystać z innej ścieżki
        - istnieją narzędzia nie wymagające umiejętności programowania. Choć nadal należy rozumieć
        co się robi, co jest możliwe a co niekoniecznie. NLP jest bardzo dynamicznie
        rozwiającą się technologią, ale stopień jej rozwoju jest inny dla każdego języka
        naturalnego (to co dziś jest możliwe dla
        tekstu angielskiego, niekoniecznie będzie możliwe dla tekstu polskiego).
        </p>

        <p>
        Jednym z popularnych
        narzędzi wśród humanistów cyfrowych jest TEI Publisher
        <label for="9-1" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="9-1" class="margin-toggle"/>
        <span class="sidenote">Oficjalna strona aplikacji
        <a href="https://teipublisher.com/index.html" target="_blank" rel="noopener">TEI Publisher</a>.
        </span>
        , aplikacja znacznie ułatwiająca
        przygotowanie edycji cyfrowych na bazie dokumentów xml w standardzie TEI. Program ten od
        jakiegoś czasu posiadał możliwość ręcznej anotacji tekstu podstawowymi eykietami typu <em>osoba</em>
        i <em>miejsce</em>
        <label for="9-8" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="9-8" class="margin-toggle"/>
        <span class="sidenote">Zobacz video:
        <a href="https://www.youtube.com/watch?v=5eVilDacrtA" target="_blank" rel="noopener">TEI Publisher
        Annotation Facility: Flexible, Browser Based Annotations for TEI</a>.
        </span>.
        Od wersji 8.00 (jeszcze nie opublikowanej) TEI Publisher będzie miał także możliwość anotacji
        automatycznej przy użyciu metod NLP (NER), a wykorzystuje w tym celu bibliotekę spaCy
        <label for="9-2" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="9-2" class="margin-toggle"/>
        <span class="sidenote">Artykuł na blogu
        <a href="https://e-editiones.org/names-sell-named-entity-recognition-in-tei-publisher/" target="_blank" rel="noopener">e-editiones</a>.
        </span>.
        </p>

        <p>
        Obecnie jedyną metodą przetestowania rozwojowej wersji TEI Publishera jest jego instalacja we
        własnym zakresie. Autorzy aplikacji udostępniają jednak repozytorium
        <label for="9-3" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="9-3" class="margin-toggle"/>
        <span class="sidenote">Zobacz repozytorium
        <a href="https://github.com/eeditiones/teipublisher-docker-compose" target="_blank" rel="noopener">eeditiones</a>.
        </span>
        z przygotowanymi plikami konfiguracyjnymi do utworzenia i
        uruchomienia własnej instancji opartej na kontenerach dockera. Ten sposób
        instalacji uwalnia nas od potrzeby
        ręcznego konfigurowania wszystkich komponentów: javy, pythona, eXist-db, spaCy itd.

        Po pobraniu (sklonowaniu) repozytorium należy uruchomić budowę obrazów dockera ze źródeł
        poleceniem:<br>
        <code>docker-compose build --build-arg ADMIN_PASS=my_pass</code><br>
        gdzie 'my_pass' oznacza hasło admina serwera eXist-db.
        </p>

        <p>
        Budowanie może potrwać kilka minut, po jego zakończeniu można uruchomić rozwojową wersję
        aplikacji poleceniem: <code>docker-compose up -d</code>, co znów zajmie kilka-kilkanaście
        sekund, po czym aplikacja będzie widoczna w przeglądarce pod adresem
        <code>http://localhost</code>.
        Co prawda w stopce strony będzie ciągle widoczna wersja 7.10, ale w rzeczywistości działa
        wersja rozwojowa z gałęzi <em>master</em> repozytorium. Aby się o tym przekonać wystarczy wejść w kolekcję
        dokumentów 'Annotation Samples' dostarczoną z aplikacją, następnie w dowolny dokument,
        np. <em>Letter #20 from Robert Graves to William Graves (at Oundle School) November 8,
        1957</em>, w pasku narzędzi po lewej stronie ekranu wśród ikon widoczna jest czarna ikonka
        z symbolem osób i plusem. To znak, że w systemie dostępna jest usługa NER. Po jej
        uruchomieniu aplikacja wyświetla okno dialogowe pozwalające wybrać model do przetwarzania
        tekstu. Standardowo dostępne są dwa: <strong>de_core_news_sm</strong> dla języka niemieckiego
        i <strong>en_code_web_sm</strong> dla języka angielskiego.
        </p>
        <img src="/static/img_zapiski/tei_publisher_ner_01.png" width="650" alt="modele spaCy w TEI Publisherze"/>

        <p>
        Przykładowy dokument jest po angielsku należy więc wybrać ten drugi model i przycisk Run.
        Po przetworzeniu można będzie zobaczyć nowe anotacje typu <em>person</em> lub <em>organization</em>.
        </p>
        <img src="/static/img_zapiski/tei_publisher_ner_02.png" width="650" alt="wynik NER w TEI Publisherze"/>

        <p>
        Przełączenie się na widok źródeł TEI dokumentu pokaże, że postacie np. Juanito
        zostały poprawnie otagowane znacznikiem <code><persName></code>.
        </p>
        <img src="/static/img_zapiski/tei_publisher_ner_03.png" width="500" alt="kod XML-TEI"/>

        <p>
        Tekst, który jest analizowany w tym cyklu zapisek jest jednak tekstem w języku polskim.
        Czy da się w podobny sposób jak przykładowy dokument przetworzyć publikację Stanisława
        Bodniaka? W nieoficjalnej wersji rozwojowej wymaga to pewnych zabiegów. W przypadku
        uruchomionej już instancji aplikacji niezbędna jest ingerencja w zawartość jednego
        z kontenerów obsługujących system TEI Publisher, ten w którym działa serwis NER oparty
        na bibliotece spaCy. Domyślnie po uruchomieniu nazywa się on
        <strong>teipublisher-docker-compose-ner-1</strong>
        co można zobaczyć na liście kontenerów uruchamiając w konsoli komendę:<br>
        <code>docker container ps</code><br>
        Do działającego kontenera można dostać się poleceniem <em>exec</em> dockera np.:<br>
        <code>docker exec -it teipublisher-docker-compose-ner-1 bash</code>,<br>
        po którego wywołaniu
        znajdziemy się w linuksowym środowisku kontenera (w tym przypadku opartym na debianie).
        To co jest niezbędne do obsługi języka polskiego to polski model spaCy, który można
        zainstalować poleceniem: <code>python -m spacy download pl_core_news_md</code>. Sama
        instalacja modelu nie spowoduje jeszcze jego dostępności na liście modeli w TEI
        Publisherze, konieczny jest jeszcze restart kontenera a najpierw wyjście z niego
        poleceniem <code>exit</code>. Do zatrzymania zestawu kontenerów obsługujących TEI
        Publishera wystarczy w linii komend wpisać: <code>docker-compose stop</code>, a po chwili
        ponownie uruchomić system poleceniem <code>docker-compose start</code>. Można by uniknąć
        konieczności modyfikowania zawartości kontenera, gdyby polski model instalowany był
        domyślnie, odpowiada za to plik konfiguracyjny <code>Dockerfile</code> w repozytorium
        serwisu NER TEI Publishera.
        <label for="9-4" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="9-4" class="margin-toggle"/>
        <span class="sidenote">Zobacz
        <a href="https://github.com/eeditiones/tei-publisher-ner/blob/master/Dockerfile" target="_blank" rel="noopener">repozytorium</a>.
        </span>

        dodanie do polecenia RUN w tym pliku dodatkowego kodu:<br>
        <code>&& python3 -m spacy download pl_core_news_md</code><br>
        rozwiązałoby sprawę.
        </p>

        <p>
        Jednak dostępność polskiego modelu na liście modeli w interfejsie użytkownika
        TEI Publishera nie oznacza niestety, że potrafi on z tego modelu skorzystać. Problemem
        są odmienne etykiety, którymi model oznacza znalezione encje nazwane (<em>named entity</em>).
        Autorzy TEI Publishera przewidzieli mapowanie z etykiet używanych w modelach angielskich
        i niemieckich np. <code>"person": ("PER", "PERSON")</code> jednak w standardowych modelach
        spaCy dla języka polskiego etykiety są nieco inne: np. 'persName' dla osób czy 'orgName'
        dla organizacji.
        <label for="9-5" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="9-5" class="margin-toggle"/>
        <span class="sidenote">zob. na stronie
        <a href="https://spacy.io/models/pl" target="_blank" rel="noopener">spaCy</a>
        , dla modelu pl_core_news_md należy rozwinąć sekcję
        <em>Label Scheme</em> i przewinąć do punktu NER, gdzie znajduje się lista etykiet
        NER używanych w tym modelu.
        </span>
        </p>

        <p>
        Aby to zmienić konieczna jest modyfikacja kodu źródłowego serwisu NER TEI Publishera,
        konkretnie pliku <code>main.py</code> w podkatalogu  <code>workspace/tei-publisher-ner/scripts</code>,
        znów w działającym kontenerze (być może w oficjalnej wersji aplikacji będzie to już
        uwzględnione?)
        <label for="9-6" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="9-6" class="margin-toggle"/>
        <span class="sidenote">Tymczasowy fork z modyfikacją jest w
        <a href="https://github.com/pjaskulski/tei-publisher-ner">repozytorium</a>.
        </span>
        . W pliku należy zmienić zawartość słownika MAPPINGS z:
        <pre>
            MAPPINGS = {
                "person": ("PER", "PERSON"),
                "place": ("LOC", "GPE"),
                "organization": ("ORG"),
                "author": ("AUT")
            }
        </pre>
        </p>

        <p>
        na:
        <pre>
            MAPPINGS = {
                "person": ("PER", "PERSON", "persName"),
                "place": ("LOC", "GPE", "placeName", "geogName"),
                "organization": ("ORG", "orgName"),
                "author": ("AUT")
            }
        </pre>

        </p>

        <p>Tym razem restart kontenerów nie będzie potrzebny, wywołanie narzędzia NER
        w TEI Publisherze wyświetla już znalezione w polskim tekście
        encje, na poniższym zrzucie ekranu widać też przygotowany przez TEI Publishera
        kod pliku XML w standardzie TEI
        <label for="9-7" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="9-7" class="margin-toggle"/>
        <span class="sidenote">Uwaga: zmiany wprowadzane bezpośrednio w kontenerach nie znikną
        po zatrzymaniu i ponownym uruchomieniu kontenera, oczywiście znikną jednak przy ponownym
        utworzeniu kontenerów z obrazów dockera.
        </span>.
        </p>
        <img src="/static/img_zapiski/tei_publisher_ner_04.png" width="650" alt="wynik NER w TEI Publisherze, tekst w języku polskim"/>

        <p>
        Można spodziewać się, że pewne niedogodności wersji rozwojowej zostaną w przyszłości
        dopracowane, a sama instalacja może być przecież przeprowadzona na serwerze przez
        specjalistę. Lektura artykułu zlinkowanego w przypisie na początku zapiski
        pokazuje także, że w rozwojowej wersji aplikacji przewidziano bardziej zaawansowane
        możliwości np. trenowanie własnego modelu w oparciu o korpus tekstów anotowanych
        właśnie w TEI Publisherze
        <label for="9-9" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="9-9" class="margin-toggle"/>
        <span class="sidenote">Inną ciekawą aplikacją obsługującą 'wzbogacanie' tagowania plików
        TEI o rozpoznane jednostki nazewnicze dzięki NER i uczeniu własnych modeli jest
        <a href="https://github.com/NEISSproject/tei_entity_enricher/wiki">NEISS - TEI Entity Enricher</a>.
        </span>.
        Aplikacja ta już dziś jest cennym narzędziem w arsenale
        cyfrowego humanisty, po ukazaniu się oficjalnej wersji 8.00 stanie się też
        systemem ułatwiającym praktyczne korzystanie z niektórych metod (NER) przetwarzania
        języka naturalnego bez potrzeby programowania i z pożytkiem w postaci łatwiejszego
        tworzenia edycji cyfrowych.
        </p>

        </section>
    </article>

    <article>
        <hr>
        <h2 id="2022-10-30">spaCy i koreferencje, czyli Jerzy bawił we Włoszech<br> a <strong>jego</strong> włości popadały w ruinę (<strong>30.10.2022</strong>) </h2>

        <section>
        <p>
        Jednym z problemów podczas przetwarzania tekstów pisanych z myślą o ludzkim
        czytelniku a nie algorytmie wykonywanym przez komputer są informacje odnoszące się 
        do tej samej postaci, ale wyrażone nie wprost, lecz choćby poprzez zaimki osobowe lub
        dzierżawcze, czyli koreferencje. Czytając na przykład tekst:

        <p>
        <em>"Jan Tarnowski, syn Jana Amora Iuniora i Barbary z Rożnowa, wnuczki Zawiszy Czarnego,
        pochodził z wpływowej szlacheckiej rodziny Leliwitów Tarnowskich...<br>
        Wychowywał się na dworach kardynała Fryderyka Jagiellończyka...<br>
        Odtąd też trwały: jego spór z siostrzeńcem Piotrem Kmitą i żale względem dworu."</em>
        <label for="10-1" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="10-1" class="margin-toggle"/>
        <span class="sidenote">Fragmenty biogramu Jana Tarnowskiego z polskiej wersji Wikipedii.
        </span>
        </p>

        <p>
        Można obawiać się, czy fakty podane w taki sposób będą czytelne dla skryptu
        przetwarzającego tekst.
        <em>Wychowywał się</em> - ale kto? <em>Jego</em> czyli kogo?
        </p>

        <p>
        Wiedza na temat tego, kto kryje się pod wyrażeniem "jego", czy kogo dotyczy czasownik
        "wychowywał się" jest szczególnie przydatna przy próbach rozpoznawania relacji między
        encjami (np. osobami) w tekście, wiedząc że chodzi o Jana Tarnowskiego, możemy uzyskać
        informację iż ważną postacią dla niego byli Piotr Kmita i Ferdynad Jagiellończyk. Czy
        spaCy jest w stanie w tym pomóc i przede wszystkim czy to będzie działać dla języka
        polskiego? Okazuje się, że tak. W repozytorium należącym do Explosion AI, firmy
        rozwijającej spaCy znajduje się dodatkowa biblioteka <strong>Coreferee</strong>,
        przeznaczona do rozwiązywania koreferencji w języku angielskim, francuskim, niemieckim
        i polskim. Instalacja opisana jest na stronie repozytorium
        <label for="10-2" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="10-2" class="margin-toggle"/>
        <span class="sidenote">Zob. repozytorium <a href="https://github.com/explosion/coreferee">
        github</a>.
        </span>,
        dokumentacja zawiera też opis działania biblioteki, wyjaśnienie decyzji podjętych
        podczas jej tworzenia
        <label for="10-3" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="10-3" class="margin-toggle"/>
        <span class="sidenote">Coreferee powstało w ramach projektu <em>Holmes</em>
        (zobacz wpis na <a href="https://explosion.ai/blog/introduction-to-holmes">
        blogu</a> Explosion AI, Holmes niestety obsługuje tylko j. angielski i niemiecki),
        co jest przyczyną pewnych specyficznych cech narzędzia, np.
        pracuje raczej na pojednycznych tokenach.
        </span>
        , także tabelę dokładności modeli dla poszczególnych języków (dla polskiego to 72-76%).
        </p>

        <p>
        Jak użyć Coreferee? Poniżej prosty program przetwarzający wspomniany fragment
        biogramu hetmana Jana Amora Tarnowskiego.
        </p>
        <img src="/static/img_zapiski/coreferee.png" width="600" alt="biblioteka coreferee"/>

        <p>Jak wydać na powyższym zrzucie ekranu sformułowania <em>Wychowywał</em>,
        <em>Otrzymał</em>, <em>zajmował</em>, <em>jego</em> zostały prawidłowo powiązane
        z encją 'Jan'
        <label for="10-4" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="10-4" class="margin-toggle"/>
        <span class="sidenote">Notebook koreferencje.ipynb do pobrania z
        <a href="https://github.com/pjaskulski/zapiski" target="_blank" rel="noopener">repozytorium</a>.
        </span>
        .
        </p>
        <p>
        <strong>Coreferee</strong> wychwytuje klastry (przykład powyżej zawiera akurat
        tylko jeden) tokenów odwołujących się do tej samej postaci, słowo
        <em>jego</em> mogłoby zostać zastąpione przez <em>Jana Tarnowskiego</em>, <em>otrzymał</em>
        przez <em>Jan Tarnowski otrzymał</em> (zob. zrzut ekranu poniżej), ale takie liczne powtórzenia byłyby męczące dla
        ludzkiego czytelnika, który świetnie sobie radzi z odczytywaniem informacji z kontekstu.
        Sprecyzowanie o kogo chodzi jest jednak niezbędne przy maszynowym przetwarzaniu treści,
        dlatego powstają narzędzia w rodzaju Coreferee.
        </p>
        <img src="/static/img_zapiski/koreferencje_przyklad.png" width="600" alt="zdanie z przetworzonymi koreferencjami"/>

        <p>Parę tygodni temu firma Explosion AI opublikowała nowy komponent biblioteki spaCy
        <strong>Coreference Resolution</strong>, szczegółowy opis tego mechanizmu jest dostępy
        na <a href="https://explosion.ai/blog/coref">blogu</a> Explosion, niestety obsługuje
        na razie tylko język angielski
        <label for="10-5" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="10-5" class="margin-toggle"/>
        <span class="sidenote">Dostępny jest też krótki film
        <a href="https://www.youtube.com/watch?v=fio3BejnRsM" target="_blank" rel="noopener">
        Coreference Resolution in spaCy</a>.
        </span>
        .
        </p>

        </section>
    </article>

    <article>
        <hr>
        <h2 id="2023-01-??">Kraken, czyli HTR to jest to (<strong>??.01.2023</strong>) </h2>

        <section>
        <p>O ile rozpoznawanie druku nie jest dziś wielkim problemem i radzą sobie z nim liczne
        programy komercyjne, jak i open source
        <label for="11-1" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="11-1" class="margin-toggle"/>
        <span class="sidenote">Istnieją także systemy przygotowane specjalnie do odczytywania starodruków
        np. OCR4all <a href="https://www.ocr4all.org/about/ocr4all" target="_blank" rel="noopener">
        ocr4all.org</a>.
        </span>
        , to rozpoznawanie pisma ręcznego, szczególnie pochodzącego
        sprzed paru lub kilku stuleci jest ciągle sporym wyzwaniem. Duży postęp w tej dziedzinie zawdzięczamy
        rozwojowi uczenia maszynowego, szczególnie zaś uczenia głębokiego. Jedną z aplikacji przeznaczonych
        do odczytywania pisma ręcznego (choć druku także) dzięki konwolucyjnym i rekurencyjnym sieciom neuronowym jest
        Kraken. Program oparty na wcześniejszym systemie OCRopus, dostępny bezpłatnie wraz z kodem źródłowym
        na otwartej licencji (Apache 2.0).
        </p>

        <p>
        Aplikację można zainstalować w systemie Linux Lub Mac OSX, procedura instalacji jest typowa
        dla pakietów języka Python i została opisana na <a href="https://kraken.re/master/index.html">stronie</a> Krakena.
        Nie jest to aplikacja z interfejsem użytkownika - obsługuje się ją z linii komend.
        Obliczenia wykonywane podczas tzw. treningu modeli - sieci neuronowych ropoznających pismo,
        są dosyć czasochłonne. Kraken potrafi skorzystać z mocy obliczeniowej karty graficznej komputera,
        o ile taka jest dostępna (chodzi o dedykowaną kartę GPU np. NVIDIA GeForce). Brak takiej
        karty nie uniemożliwia skorzystania z aplikacji, wydłuża jednak czas trenowania własnych modeli.
        </p>

        <p>Kraken jest wykorzystywany do rozpoznawania wielu rodzajów pisma w wielu językach,
        skryptów pisanych od lewej do prawej - jak w piśmie europejskim, od prawej do lewej
        (jak w hebrajskim czy arabskim), czy od góry do dołu. W przypadku braku gotowego modelu
        do rozpoznawania jakiegoś pisma, użytkownik może przygotować własny, operając się na materiałach
        treningowych - skanach rękopisów i manualnie przygotowanej transkrypcji.
        Czym w ogóle są modele? Wiedza na temat rozpoznawanego pisma nie jest zaszyta w aplikacji, lecz
        zapisana w binarnch pilkach w formacie *.mlmodel. Plik taki, zwykle o objętości 15-25 MB zawira
        szczegółową definicję warstw sieci neuronowej wytrenowanej wcześniej do rozpoznawania danego
        pisma. Kraken wczytując dane takiego modelu jest w stanie przetworzyć skan rękopisu i zwrócić
        rozpoznany tekst. Istnieje katalog gotowych modeli udostępnionych na otwartej licencji
        przez ich autorów (zob. <a href="https://zenodo.org/communities/ocr_models?page=1&size=20">zenodo</a>).
        Są tam m.in. modele rozpoznające pismo średniowieczne z XII-XV wieku: 'HTR Model - Medieval Latin and
        French 12th-15th c. expanded (no abbr.)' czy model rozpoznający fransuskie pismo ręczne z XVIII-XX wieku
        'LECTAUREP Contemporary French Model (Administration)' trenowany na dokumentach notarialnych.
        Jeżeli posiadamy rękopisy zbliżone materiałów na których trenowano jeden z dostępnych publicznie
        modeli możemy go wykorzystać do rozpoznawania prawdopodobnie z niezłym rezultatem.
        </p>

        <p>Kraken posiada wbudowaną obsługę pobierania modeli z katalogu na stronie zenodo.org.
        Polecenie <code>kraken list</code> wyświetla listę modeli dostępnych do pobrania, wśród wyświetlanych
        informacji jest identyfikator pliku np. '10.5281/zenodo.6542744' (to wspomniany wcześniej model
        oparty na pismach notarialnych). Kolejne polecenie <code>kraken get</code> w którym należy wskazać
        taki identyfikator pobiera model na dysk użytkownika (np. do katalogu ~/.config/kraken w systemie Ubuntu).
        </p>

        <p>Pobrany model może być od razu użyty do rozpoznawania pisma. W przykładowym poleceniu:</p>

        <pre>kraken -i image.png image_text.txt segment -bl ocr -m HTR-United-Manu_McFrench.mlmodel</pre>

        <p>parametr <code>-i image.png image_text.txt</code> oznacza nazwę wejściowego pliku ze skanem
        rękopisu, oraz nazwę wyjściowego pliku tekstowego, <code>segment -bl</code> oznacza przygotowanie
        segmentacji, podziału skanu na regiony i rozpoznaniu wierszy, zaś
        <code>ocr -m HTR-United-Manu_McFrench.mlmodel</code>
        jest poleceniem użycia do rozpoznawania pisma jednego z wcześniej pobranych modeli.
        </p>

        <p><em>c.d.n.</em></p>
        </section>
    </article>

    <article>
    <hr>
        <h2 id="2023-01-29">Dżin z czarodziejskiej lampy czyli GPT 3.5 (<strong>29.01.2023</strong>) </h2>

        <section>
        <p>Kiedy w listopadzie 2022 roku firma OpenAI
        <label for="12-1" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="12-1" class="margin-toggle"/>
        <span class="sidenote">Strona firmy <a href="https://www.openai.com" target="_blank" rel="noopener">openai.com</a>.
        </span>
        udostępniła publicznie wersję beta
        swojego nowego produktu ChatGPT potrzeba było zaledwie kilku dni by liczba
        jego użytkowników osiągnęła milion. Dziś jest ich z pewnością wielokrotnie więcej,
        perspektywa porozmawiania ze sztuczną inteligencją lub zlecenia jej jakiejś pracy
        jest trudna do odparcia. Jakie jednak praktyczne znaczenie może mieć ChatGPT
        czy ogólnie duże modele językowe (LLM) dla cyfrowego historyka?</p>

        <p>Czym w ogóle jest GPT (Generative Pretrained Transformer), którego najnowszą
        odsłonę okresla się często jako wersję GPT-3.5 (nazywaną też InstructGPT)? To
        właśnie tzw. duży model językowy (LLM)
        <label for="12-7" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="12-7" class="margin-toggle"/>
        <span class="sidenote">zob. <em>Language model</em>: <a href="https://en.wikipedia.org/wiki/Language_model" target="_blank" rel="noopener">Wikipedia</a>.
        </span>
        wytrenowany przez firmę OpenAI na bazie ogromnego zbioru tekstów oraz udoskonalony
        poprzez uczenie wzmacniające na podstawie informacji zwrotnych od ludzi
        <label for="12-10" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="12-10" class="margin-toggle"/>
        <span class="sidenote"><a href="https://openai.com/blog/instruction-following/" target="_blank" rel="noopener">Aligning Language Models to Follow Instructions</a>.
        </span>.
        Modele tego typu
        są używane do generowania tekstów, tłumaczenia,
        tworzenia streszczeń, ale też do zadań wymagających pewnego rozumienia tekstu.
        Można na przykład poprosić model o przygotowanie konspektu artykułu na jakiś temat,
        oraz listy z podstawową bibliografią, internet jest już pełen artykułów i filmów na temat
        tego jak ChatGPT wpłynie na pracę akademicką i badania naukowe
        <label for="12-8" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="12-8" class="margin-toggle"/>
        <span class="sidenote">zob. <a href="https://www.lumiere-education.com/post/how-will-chatgpt-change-research-paper-writing"  target="_blank" rel="noopener">How Will ChatGPT Change Research Paper Writing?</a>,
        <a href="https://www.youtube.com/watch?v=_wZkRXZXFIM" target="_blank" rel="noopener">ChatGPT: A game changer for researchers?</a>,
        <a href="https://www.youtube.com/watch?v=pwUhj7j7xDw" target="_blank" rel="noopener">Harness the power of AI for research: How ChatGPT is changing the game</a>.
        </span>. Trzeba jednak brać pod uwagę, że jest to bardzo wczesny etap fascynacji nową technologią
        i zarówno wiele możliwości wykorzystania jak i wiele pułapek związanych z wykorzystaniem GPT/LLM
        nie zostało jeszcze rozpoznanych.
        Pytanie, na które dziś można spróbować odpowiedzić, brzmi: czy tak zaawansowany model, rozumiejący
        całkiem dobrze język polski, mógłby wykonywać zadania związane z masowym przetwarzaniem
        tekstów publikacji i źródeł historycznych?
        </p>

        <p>ChatGPT dostępny jest (obecnie) jedynie w formie interaktywnej. Można z nim porozmawiać
        o historii, sprawdzić poziom wiedzy, a równie często też niewiedzy (czy Jan Amor Tarnowski
        był konfederatą barskim? Chyba jednak nie...). Ale z modeli GPT można również korzystać
        poprzez sprzedawane przez OpenAI usługi API
        <label for="12-2" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="12-2" class="margin-toggle"/>
        <span class="sidenote">API jest usługą płatną, jednakże każdy nowy użytkownik otrzymuje
        bezpłatnie do wykorzystania tokeny o wartości 18$, zob. <a href="https://beta.openai.com/overview" target="_blank" rel="noopener">OpenAI API</a>.
        </span>,
        co umożliwia tworzenie prostych skryptów, które
        przetwarzają teksty z wykorzystaniem modeli o różnej mocy, a także eksperymentowanie
        z różnymi parametrami zmieniającymi ich charakterystykę. Skrypty te mogą nie tyle pytać
        model o jego wewnętrzną wiedzę, ale mogą wykorzystać umiejętności językowe LLM (Large Language Model)
        właśnie do przetwarzania podanych mu tekstów. Tak naprawdę można to zrobić także
        podczas sesji z ChatGPT, ale wykorzystanie skryptów i kolekcji tekstów jest chyba
        wygodniejsze
        <label for="12-12" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="12-12" class="margin-toggle"/>
        <span class="sidenote">Istnieje jeszcze tzw. <a href="https://platform.openai.com/playground" target="_blank" rel="noopener">OpenAI Playground</a>
        gdzie można testowac modele dostępne przez API w formie bardziej interaktywniej,
        narzędzie wymaga zalogowania się
        kontem OpenAI, a jego używanie kosztuje tyle samo co korzystanie z API.
        </span>
        .
        </p>

        <p>Jak GPT radzi sobie
        z wyszukiwaniem słów kluczowych, osób, miejsc? Czy potrafi wyszukiwać informacje o relacjach rodzinnych
        między postaciami występującymi w tekście? Rozpoznać funkcje, stanowiska i urzędy pełnione przez głównego
        bohatera przesłanej publikacji? Jak model sprawdzi się w roli narzędzia automatycznie przetwarzającego
        całe pakiety tekstów w celu wydobycia kluczowych informacji?
        <label for="12-3" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="12-3" class="margin-toggle"/>
        <span class="sidenote">Takie i inne eksperymenty opisuję także na stronie repozytorium:
        <a href="https://github.com/pjaskulski/gpt_historical_text">gpt_historical_text</a>.
        </span>
        </p>

        <p>Czy GPT poradzi sobie na przykład z wyciągnięciem z publikacji Stanisława Bodniaka
        listy osób?
        Po przesłaniu do modelu kilku akapitów tekstu (<em>"Polska a Bałtyk za ostatniego
        Jagiellona"</em>
        <label for="12-3" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="12-3" class="margin-toggle"/>
        <span class="sidenote">Wybrany <a href="https://github.com/pjaskulski/gpt_historical_text#analiza-ner-fragmentu-publikacji" target="_blank" rel="noopener">fragment</a>.
        </span>)
        oraz zapytania (promptu):<br><br>

       <strong>prompt</strong>: <em>W podanym tekście wyszukaj informacje o osobach, znalezione osoby
       wypisz w formie listy, jeżeli są znane funkcje lub urzędy osób
       umieść te informacje w nawiasach obok osób</em>
        </p>

        <p>
        model <strong>text-davnci-003</strong> (uruchomiony z parametrami: <code>temperature=0.0</code>,
        <code>top_p=1.0</code>, <code>frequency_penalty=0.0</code>,
        <code>presence_penalty=0.0</code>) zwraca całkiem niezły wynik:
        <ul>
            <li>- Zygmunt August (król)</li><br>
            <li>- Jan Tarnowski (hetman)</li><br>
            <li>- Piotr Kmita (marszałek koronny)</li><br>
            <li>- Stanisław Hozjusz (bp)</li><br>
            <li>- Jan Ocieski (kanclerz)</li><br>
            <li>- Jan Przerębski (podkanclerzy)</li><br>
            <li>- Marcin Kromer</li><br>
            <li>- Szymon Maricjus-Czystochlebski</li><br>
            <li>- Łukasz Górnicki</li><br>
            <li>- Barnim XI (książę)</li>
        </ul>
        </p>

        <p>pomijając jedynie ks. Albrechta.</p>

        <p>Inny przykład:<br>
        <strong>prompt</strong>: <em>W podanym tekście wyszukaj słowa kluczowe będące nazwami własnymi, znalezione słowa kluczowe
         wypisz w formie listy, w nawiasach obok słowa kluczowego umieść informację
         o typie słowa kluczowego np. osoba, miejsce, rzeka, morze.</em>
        </p>
        <p>
        Wynik:
            <ul>
                <li>- Zygmunt August (osoba)</li><br>
                <li>- Malbork (miejsce)</li><br>
                <li>- Królewiec (miejsce)</li><br>
                <li>- Albrecht (osoba)</li><br>
                <li>- Jan Tarnowski (osoba)</li><br>
                <li>- Piotr Kmita (osoba)</li><br>
                <li>- Stanisław Hozjusz (osoba)</li><br>
                <li>- Jan Ocieski (osoba)</li><br>
                <li>- Jan Przerębski (osoba)</li><br>
                <li>- Marcin Kromer (osoba)</li><br>
                <li>- Szymon Maricjus-Czystochlebski (osoba)</li><br>
                <li>- Łukasz Górnicki (osoba)</li><br>
                <li>- Gdańsk (miejsce)</li><br>
                <li>- Wisła (rzeka)</li><br>
                <li>- Barnim XI (osoba)</li><br>
                <li>- Lębork (miejsce)</li><br>
                <li>- Bytów (miejsce)</li><br>
                <li>- Bałtyk (morze)</li><br>
            </ul>
        </p>

        <p>
        A może coś bardziej skomplikowanego? Na przykład czy model GPT mógłby na podstawie
        przekazanego tekstu ustalić powiązania rodzinne jego głównego bohatera? Do testu wykorzystany
        zostanie artykuł na temat Ludwika Wejhera pochodzący z Wikipedii
        <label for="12-4" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="12-4" class="margin-toggle"/>
        <span class="sidenote">
        <a href="https://pl.wikipedia.org/wiki/Ludwik_Wejher_(zm._1656)" target="_blank" rel="noopener">Ludwik Wejher (zm. 1656)</a> wojewoda pomorski.
        </span>
       . Zapytanie (podpowiedź), która ma skłonić model 'davinci' do poprawnej odpowiedzi
        wygląda tym razem tak:
        </p>

        <p>
        <strong>prompt</strong>: <em>Na podstawie podanego tekstu wyszukaj
        wszystkie relacje rodzinne głównego bohatera (tylko jego krewnych, powinowatych,
        teściów, szwagrów, szwagierek). Wynik wypisz w formie listy nienumerowanej
        z rodzajem pokrewieństwa w nawiasie. Na przykład:<br>
        - Jan Kowalski (brat)<br>
        - Anna (siostra)<br>
        Jeżeli w tekście nie ma takich informacji napisz: brak danych.</em>
        </p>

        <p>Wynik:<br>
        <ul>
        <li>- Jan Wejher (ojciec)</li><br>
        <li>- Anna ze Szczawińskich (matka)</li><br>
        <li>- Mikołaj Wejher (brat)</li><br>
        <li>- Jakub Wejher (brat)</li><br>
        <li>- Gerard Denhoff (teść)</li><br>
        <li>- Katarzyna Jadwiga z Denhoffów (żona)</li><br>
        <li>- brak danych (szwagrów, szwagierek)</li><br>
        </ul>
        </p>

        <p>Przykład listy stanowisk lub urzędów dla tej samej postaci (i z
        tego samego tekstu):</p>

        <p><strong>Prompt</strong>: <em>Na podstawie podanego tekstu wymień wszystkie
        stanowiska i urzędy głównego bohatera tekstu. Wynik wypisz w formie listy.</em>
        </p>

        <p>
        <ol>
            <li>1. Pułkownik od 1647</li><br>
            <li>2. Kasztelan elbąski od 1648</li><br>
            <li>3. Wojewoda pomorski od 1649</li><br>
            <li>4. Starosta skarżewski</li><br>
            <li>5. Starosta wałecki</li><br>
            <li>6. Starosta nowodworski w 1650 roku</li><br>
        </ol>
        </p>

        <p>Jak w ogóle model językowy to robi, jak radzi sobie z zadaniami z zakresu NLP,
        użyte powyżej zapytania albo nie zawierają w ogóle przykładów albo jedynie
        proste przykłady wyników? Możliwości modelu GPT wynikają z rozmiarów architektury
        sieci (175 miliardów parametrów) oraz rozmiarów danych treningowych, użytych do
        uczenia - zbiór tekstów liczył około biliona słów
        <label for="12-14" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="12-14" class="margin-toggle"/>
        <span class="sidenote">
        Jak wygląda i działa architektura
        transformerów np. modelu GPT można przeczytać w artykule na blogu Mirosława Mamczura
        <a href="https://miroslawmamczur.pl/czym-jest-i-jak-dziala-transformer-siec-neuronowa/">
        Czym jest i jak działa transformer (sieć neuronowa)?</a>
        </span>.
        Niektóre z cech modelu GPT-3 były niespodzianką
        nawet dla jego twórców na przykład umiejętność wykonywania poleceń w języku innym niż angielski
        <label for="12-13" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="12-13" class="margin-toggle"/>
        <span class="sidenote">
        <a href="https://twitter.com/janleike/status/1625207251630960640">Jan Leike tweet</a>
        </span>
        . Mimo, że 93% użytego do trenowaia GPT-3 materiału uczącego to teksty w języku
        angielskim, model `davinci-003` radzi sobie całkiem dobrze także z innymi językami
        np. z polskim, a materiały polskie stanowiły zaledwie 0.25% całości!
        </p>

        <p><strong>Skoro jest tak dobrze... a może wcale nie jest?</strong></p>

        <p>Modele GPT-3.5 takie jak 'davinci' są bardzo dobrymi modelami językowymi, nie są jednak
        sztuczną inteligencją, język nie jest przecież tożsamy z myśleniem. Wiedza modelu
        trenowanego wieloma gigabajtami tekstów również nie jest całkiem pewna i ścisła.
        Pytany o fakty model 'davinci' (lub ChatGPT) jest często całkiem poprawny w swoich
        odpowiedziach, ale potrafi fakty tworzyć, czasami wbrew zwykłej logice, na przykład
        wspomniane wyżej rzekome uczestnictwo hetmana Tarnowskiego w konfederacji barskiej,
        mimo iż żył ponad 200 lat wcześniej i nie było to możliwe.
        <label for="12-9" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="12-9" class="margin-toggle"/>
        <span class="sidenote">
        Innym problemem są ograniczenia techniczne, model może przetworzyć w jednym
        zapytaniu 4000 tokenów, przy czym chodzi o sumę długości zapytania i wyniku. A ponieważ token
        to często mniej niż wyraz (dłuższe wyrazy dzielone są na kilka tokenów), to realny
        tekst możliwy do jednorazowego przetworzenia to np. 6 tys. znaków, przy założeniu
        odpowiedzi o długości 500-800 tokenów. Dłuższe teksty wymagałyby albo wstępnego
        skrócenia, albo podzielenia.
        </span>
        </p>

        <p>Nie przeszkadza to przy generowaniu tekstów, udzielaniu łudząco przypominających
        ludzkie odpowiedzi na zadanie pytania, jednak przy wyciąganiu informacji np.
        z przekazanych modelowi biografii oczekiwana jest precyzja, powtarzalność wyników,
        pewność informacji. Model, zależnie od parametrów uruchomienia, może mieć skłonnośc
        do większego lub mniejszego interpretowania danych i zmyślania faktów, zjawisko to
        określa się czasem mianem halucynacji. Może też na przykład udzielić odpowiedzi wynikającej
        z kontekstu - o ile dotyczy to informacji, że syn córki bohatera biografii jest jego wnukiem,
        to jest cenna umiejętność. Natomiast ustalanie miejsca urodzenia osoby na
        podstawie informacji, iż bohater pochodził z rodu wywodzącego się z miejscowości X,
        jest już przesadną interpretacją danych, która może, ale nie musi być prawdą.
        </p>

        <p>Próby wykorzystania dużych modeli językowych do ekstrakcji informacji z tekstów
        napotykają często problem, którego źródłem jest sam przetwarzany tekst. Język używany
        w opracowaniach bywa po prostu trudny do interpretacji dla modelu. Przykładem może być 
        biogram Mariana Świeykowskiego, żyjącego w XIX wieku marszałka szlachty guberni wołyńskiej,
        pochodzący z Polskiego Słownika Biograficznego (PSB t. 51, 2016-17, s. 377-379)
        <label for="12-5" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="12-5" class="margin-toggle"/>
        <span class="sidenote">
        Polski Słowik Biograficzny dostępny jest online w repozytoriach <a href="https://rcin.org.pl/ihpan/dlibra/publication/79352/edition/65205">
        RCIN</a>, aczkolwiek jest to dostęp limitowany.
        </span>
        . Człowiek
        nie będzie miał problemu z interpretacją informacji podanych w tym tekście, jednak 'davinci-003'
        poproszony o wypisanie krewnych i powinowatych, wraz z rodzajem relacji rodzinnych,
        popełnia uparcie błąd dla jednej z 12 spokrewnionych osób, nazywając Rocha Kossowskiego
        babcią bohatera biogramu. Można podejrzewać, że wynika to z konstrukcji zdania i przyjętego
        w PSB sposobu zapisywania informacji z unikaniem powtarzania nazwisk czy stosowaniem skrótów.
        Problematyczne zdanie brzmi: <em>"Ur. prawdopodobnie w Kołodnie (pow. krzemieniecki), był wnukiem
        Leonarda (Leona) Marcina (zob.) i Rocha Kossowskiego (zob.), synem Adama Macieja (ok. 1780 — 1858),
        kapitana 1. p. huzarów wojsk[...]"</em> gdzie mamy wymienionych dwóch dziadków bohatera,
        w tym pierwszy z nich bez podania nazwiska (w domyśle Świeykowski).
        </p>

        <p>Po uproszczeniu tekstu (w tym powyższego fragmentu do:<em>"Był wnukiem Leonarda Marcina
        Świeykowskiego a także Rocha Kossowskiego."</em>) model zwraca całkowicie poprawne
        dane krewnych Mariana Świeykowskiego. Tym razem zapytanie zadano w języku angielskim,
        stąd angielskie nazwy rodzajów pokrewieństwa.</p>

        <p><strong>Prompt</strong>: <em>Based on the given text, search for
         all relatives and affinities of the main character. List the result in
         the form of an unnumbered list with the type of kinship in parentheses.
         For example:<br>
         - John Kowalski (brother)<br>
         - Anna (sister)<br>
         If there is no information in the text about some types of kinship omit
         these types.</em>
        </p>

        <p>
        Wynik:<br>
        <ul>
        <li>- Adam Maciej Świeykowski (father)</li><br>
        <li>- Krystyna Świeykowski (mother)</li><br>
        <li>- Leonard Marcin Świeykowski (grandfather)</li><br>
        <li>- Roch Kossowski (grandfather)</li><br>
        <li>- Olga Świeykowski (sister)</li><br>
        <li>- Cezary Poniatowski (brother-in-law)</li><br>
        <li>- Jadwiga Świeykowski (sister)</li><br>
        <li>- Izabela Janina Sobańska (wife)</li><br>
        <li>- Hieronim Sobański (father-in-law)</li><br>
        <li>- Olga Maria Zofia Świeykowski (daughter)</li><br>
        <li>- Stefan Grocholski (son-in-law)</li><br>
        <li>- Maria Świeykowski (daughter)</li><br>
        <li>- Leon Ledóchowski (son-in-law)</li><br>
        </ul>
        </p>

        <p>
        A przecież podczas automatycznego przetwarzania tekstów poszukujemy faktów,
        'automatyczność' wyklucza z góry jakąś wstępną interpretacje przez człowieka,
        chodzi właśnie o przeniesienie pracy na narzędzia informatyczne, by uzyskane dane
        szybciej i sprawniej wykorzystać np. do tworzenia baz wiedzy.
        <label for="12-5" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="12-5" class="margin-toggle"/>
        <span class="sidenote">
        Czy model językowy nie mógłby najpierw uprościć, streścić przetwarzanego tekstu
        a dopiero potem przystąpić do ekstrakcji informacji? Niestety, należy się spodziewać
        że błędy interpretacyjne, które model popełnia, przeniesie po prostu do uproszczonej wersji.
        </span>
        .
        Problem lepszego przetwarzania bardziej zawiłych tekstów być może zostanie rozwiązany
        w kolejnych wersjach modeli LLM, plotki głoszą, że GPT-4 pojawi się już w tym roku
        (i że to ta nowa wersja obsługuje niedawno zaprezentowaną przez Microsoft integrację 
        AI z wyszukiwarką Bing)
        <label for="12-11" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="12-11" class="margin-toggle"/>
        <span class="sidenote">Kiedy rozpoczęto prace nad integracją GPT z githubem, początkowo
        model w 70% przypadków generował błędny kod, dziś wielu programistów nie wyobraża sobie
        pracy bez usługi Github Copilot, zob. <a href="https://www.youtube.com/watch?v=lnufceCxwG0">wywiad z Natem Friedmanem</a>.
        </span>
        .
        Z kolei przed nadinterpretacją lub skłonnościami 'halucynacyjnymi' modelu można się 
        częściowo zabezpieczać używając odpowiednio jego parametrów, można też zadbać
        o jak najlepszy tekst zapytania (promptu).
        Wyniki działania modeli będą jednak zawsze wymagały weryfikacji. GPT-3.5 nie jest więc
        narzędziem bezproblemowym, ale czy dżiny zamieszkujące czarodziejskie lampy nie
        bywały kłopotliwe?</p>
        </section>
    </article>

    <article>
    <hr>
        <h2 id="2023-06-08">Podręczny OCR. Jak odczytywać fragmenty tekstu z obrazów (<strong>08.06.2023</strong>) </h2>

        <section>
        <p>
        Skopiowanie tekstu nie jest problemem w przypadku współczesnych plików pdf, czy stron
        internetowych, ciągle jednak spotyka się źródła zapisane w fomie pdf-ów bez warstwy
        tekstowej, czy też w plikach graficznych w rodzaju jpg, djvu. Można je oczywiście poddać 
        pełnemu rozpoznawaniu OCR w jednym z popularnych programów przeznaczonych do tego celu
        jak FineReader, jednak często potrzebujemy tylko małego fragmentu tekstu i przydałoby
        się spontanicznie zaznaczyć go na ekranie komputera i skopiować tekst do schowka.
        </p>

        <p>Z pomocą przychodzą narzędzia komercyjne w rodzaju ABBY ScreenReader, ale także liczne
        programy typu opensource, oparte najczęściej na popularnym silniku ocr - tesseract.
        Jednym z nich jest <a href="https://github.com/ianzhao05/textshot">textshot</a>, którego używam w systemie Ubuntu. Jego instalacja nie jest
        najprostsza, chyba że na codzień używa się języka programowania Python, wówczas instalacja
        przebiega podobnie do zwykłych bibliotek rozszerzających możliwości Pythona:
        <pre>pip install textshot</pre>

        <p>Wcześniej należy zadbać o instalację samego tesseracta, oraz jego pakietów
        językowych np.:</p>

        <pre>sudo apt install tesseract-ocr</pre>
        <pre>sudo apt install tesseract-ocr-all</pre>

        <p>Powyższe polecenia zainstalują silnik ocr i wszystkie dostępne języki, listę języków
        można wyświetlić poprzez komendę:</p>

        <pre>tesseract --list-langs</pre>

        <p>Samą aplikację można uruchomić z linii poleceń, dodając język (lub języki) tekstu,
        który będzie rozpoznawany:</p>

        <pre>textshot pol</pre>

        <p>
        Ale dużo praktyczniej jest podpiać jej wywołanie pod skrót klawiszowy w systemie
        (w Ubuntu Settings -> Keyboard Shortcuts). Po uruchomieniu ekran komputera ulega
        przyciemnieniu a za pomocą myszy można zaznaczyć fragment, który ma zostać rozpoznany
        przez OCR. Proces rozpoznawania przeprowadzany jest lokalnie na komputerze użytkownika
        i nie trwa długo, po chwili wyświetlane jest powiadomienie, a odczytany tekst trafia
        do systemowego schowka, skąd można go wkleić do pola bazy danych, czy też do notatnika.
        </p>
        <img src="/static/img_zapiski/textshot_in_action.png" width="600" alt="textshot w akcji"/>

        <p>
        Powyżej ilustracja z przykładem rozpoznawania fragmentu pliku w formacje djvu, z rozpoznanym
        już tekstem w edytorze tekstowym.
        </p>

        <p>
        Czy textshot będzie działać w systemie Windows? Prawdopodobnie tak, ale istnieją też inne,
        prostsze w instalacji programy, również wykorzystujące tesseract ocr, np.
        <a href="https://github.com/danpla/dpscreenocr">dpScreenOCR</a>, czy też korzystający
        do rozpoznawania ocr z API systemu Windows <a href="https://learn.microsoft.com/en-us/windows/powertoys/text-extractor">
        TextExtractor</a>, będący częścią serii PowerToys.
        </section>
    </article>

    <article>
    <hr>
        <h2 id="2023-07-02">Duże modele językowe a grafy wiedzy (<strong>02.07.2023</strong>) </h2>
        <section>
        <p align="right">
        <em>"The right way to think of the models that we create is a reasoning engine,
        not a fact database. They can also act as a fact database, but that's not really
        what's special about them – what we want them to do is something closer to the
        ability to reason, not to memorize."</em><br>
        Sam Altman, OpenAI
        <label for="14-1" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="14-1" class="margin-toggle"/>
        <span class="sidenote">
        <a href="https://abcnews.go.com/Technology/openai-ceo-sam-altman-ai-reshape-society-acknowledges/story?id=97897122" target="_blank" rel="noopener">
        Zob. wywiad w ABS News: "OpenAI CEO Sam Altman says..."</a>.
        </span>
        </p>

        <p>
        Pojawienie się sztucznej inteligencji (AI), z którą można rozmawiać w języku naturalnym
        wywołało naturalny odruch testowania jej wiedzy, skłonność do traktowania modeli GPT trochę
        jak inteligentnych komputerów z książek i filmów S&F. Dość szybko jednak pojawiło się
        rozczarowanie, bowiem duże modele językowe (LLM) nie wiedzą wszystkiego a jeżeli nie mają
        jakichś informacji bardzo chętnie je tworzą, czyli zmyślają (problem tzw. halucynacji).
        W końcu są przecież modelami generującymi treści i w tym są naprawdę dobre.
        </p>

        <p>
        Wiedza i fakty są natomiast domeną encyklopedii (np. Wikipedii), a w szczególności baz
        (grafów) wiedzy (knowledge graph - KG), takich jak wikidata.org. Można w ciągu ostatnich
        kilku miesięcy intensywnego rowoju AI zauwaźyć kilka trendów próbujących łączyć te dwa
        światy: LLM i grafy wiedzy.
        Dość oczywistym pomysłem było zaprzęgnięcie modeli GPT do generowania zapytań w języku
        SPARQL - języku zapytań wykorzystywanym w m.in. Wikidata Query Service (WQS), który -
        szczególnie dla humanistów - nie jest technologią prostą i łatwą w użyciu. I skorzystanie
        ze zdolności modelu językowego do przełożenia pomysłów w języku naturalnym na zapytanie
        w SPARQL jest dziś możliwe, model GPT-4 generuje je całkiem poprawnie. Poproszony na
        przykład o przygotowanie zapytania zwracającego listę krółów Polski, z ograniczeniem
        do tych, którzy zmarli między rokiem 1300 a 1650, przygotowuje działający kod SPARQL
        gotowy do wklejenia w WQS:
        </p>

        <pre>
SELECT ?król ?królLabel ?data_śmierci
WHERE
{
  ?król wdt:P39 wd:Q3273712.
  ?król wdt:P570 ?data_śmierci.
  FILTER(YEAR(?data_śmierci) >= 1300 && YEAR(?data_śmierci) <= 1650).
  SERVICE wikibase:label { bd:serviceParam wikibase:language "[AUTO_LANGUAGE],pl". }
}
ORDER BY ?data_śmierci
</pre>

        <p>Gotowe do uruchomienia zapytanie: <a href="https://w.wiki/6wT6" target="_blank" rel="noopener">link</a></p>

        <p>
        Integracja LLM jako silnika generującego kod SPARQL na podstawie zwykłych pytań
        użytkowników jest więc możliwa i jest przedmiotem badań firm i instytucji zajmujących
        się bazami wiedzy. Ciekawy artykuł na ten temat można przeczytać między innymi na
        stronie firmy wisecube.ai
        <label for="14-1" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="14-1" class="margin-toggle"/>
        <span class="sidenote">
        "SPARQL queries, GPTs and Large Language Models – where are
        we currently?" (<a href="https://www.wisecube.ai/blog/sparql-queries-gpts-and-large-language-models-where-are-we-currently/" target="_blank" rel="noopener">
        Link</a>).
        </span>
        Jednym z wyzwań jest zaznajomienie modelu ze strukturą danych grafu wiedzy, np.
        właściwościami i podstawowymi elementami w wikidata.org (te GPT częściowo zna,
        ale nie zna struktury np. niezależnych instancji wikibase). Innym weryfikacja poprawności
        wygenerowanego zapytania, które może zawierać nie tylko łatwe do wychwycenia błędy formalne,
        skutkujące błędem wykonania (niełatwe z kolei do poprawienia przez osobę słabo znającą SPARQL),
        ale także zapytania formalnie poprawne lecz zwracające nie całkem poprawne i oczekiwane
        wyniki. Można jednak spodziewać się, że takie mechanizmy będą w niedługim czasie
        dostępne i popularne, uzupełniając możliwości oferowane przez wizualne kreatory zapytań
        w rodzaju Sparnatural czy Wisecube Visual Query Builder.
        </p>

        <p>
        Na ostatniej konferencji "Knowledge Graph Conference 2023" Denny Vrandečić (wikidata.org)
        przedstawił wykład o tytule "The Future of Knowledge Graphs in a World of Large Language
        Models", którego omówienie wrzucił też na <a href="https://www.youtube.com/watch?v=WqYBx2gB6vA" target="_blank" rel="noopener">Youtube</a>,
        warto posłuchać jak możliwości efektywnego wykorzystania modeli językowych wyglądają
        z punktu widzenia twórców największej publicznej bazy wiedzy.
        </p>

        <p>
        Jednym z pomysłów testowanych przez specjalistów od KG są próby douczania
        (fine-tuning) modeli językowych na podstawie faktów zawartych w grafach wiedzy.
        Podejście takie może być w pewnym stopniu skuteczne
        <label for="14-1" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="14-1" class="margin-toggle"/>
        <span class="sidenote">
        Można o nim poczytać w artykule: "Large Language Model = Knowledge Graph Store? Yes, by Fine-Tuning LLM With KG"
        (<a href="https://betterprogramming.pub/large-language-model-knowledge-graph-store-yes-by-fine-tuning-llm-with-kg-f88b556959e6" target="_blank" rel="noopener">
        Link</a>).
        </span>, jednak na dłuższą metę wydaje się niepraktyczne. Pierwszym problemem
        jest koszt procesu douczania później koszt korzystania z douczonego modelu. Jeżeli
        mowa o skorzystaniu z modeli OpenAI dochodzi do tego jakość modelu bazowego,
        firma udostępnia do douczania jedynie starsze ze swoich modeli, czyli w najlepszym
        przypadku gpt-3 o wyraźnie gorszych możliwościach niż gpt-4, natomiast cena
        korzystania z douczonego już modelu będzie wyraźnie wyższa niż w przypadku
        standardowego (lepszego) gpt-4. Innym problemem jest samo douczanie, dane
        w grafach wiedzy zmieniają się, są dodawane, poprawiane, jak przeprowadzać 
        ciągłe douczanie modelu? Czy wielokrotne powtarzany fine-tuning nie spowoduje
        'zapominania' informacji nauczonych w poprzednich sesjach douczania? Czy może
        douczać model bazowy cyklicznie, na podstawie aktualizowanej bazy wiedzy?
        Obecnie trudno powiedzieć czy to byłoby wykonalne i skuteczne, na pewno zaś
        kosztowne.
        </p>

        <p>
        Innym trendem są próby udostępnienia wiedzy zawartej w grafach wiedzy dużemu
        modelowi językowemu tak, by generując odpowiedź na zapytanie użytkownika mógł
        skorzystać z faktów zawartych w grafie wiedzy a powstały wynik był dużo lepszej
        jakości, niż w przypadku korzystania tylko z wewnętrznej wiedzy modelu.
        Pomysł jest w pewnym stopniu zbliżony do stosowanych już powszechnie baz wektorowych
        zawierających osadzenia (embeddings) z korpusów tekstów/dokumentów, gdzie model
        językowy może otrzymać wyfiltrowane dane z bazy wektorowej jako element kontekstu
        zapytania i na tej podstawie udzielić odpowiedzi w języku naturalnym, nawet
        jeżeli tekst będący podstawą odpowiedzi nie był elementem korpusu uczącego
        w czasie trenowania modelu. Korzystają z takiego mechanizmu na przykład firmy,
        które mogą użyć swoich wewnętrznych dokumentów i tekstów niedostępnych publicznie,
        model LLM obsługujący formowy chat może w ten sposób stać się np. asystentem
        użytkownka skomplikowanego systemu informatycznego i mając dostęp do jego dokumentacji
        udzielać konkretnej odpowiedzi na zadawane przez użytkowników pytania.
        Można też w ten sam sposób stworzć asystenta udzielającego porad na temat
        przepisów prawnych, podpowiadającego informacje z raportów badań naukowych itp.
        Model nie musi znać tych wszystkich informacji, wykorzystywane są jedynie
        jego zdolności językowe i pewne (ograniczone na razie) umiejętności wnioskowania.
        </p>

        <p>
        Przykład takiego rozwiązania jest opisywany na blogu firmy neo4j - twórców
        grafowej bazy danych o tej samej nazwie
        (<a href="https://neo4j.com/developer-blog/context-aware-knowledge-graph-chatbot-with-gpt-4-and-neo4j/" target="_blank" rel="noopener">Link</a>).
        Artykuł omawia tworzenie chatbota, który jest w stanie skorzystać z danych
        zawartych w bazie neo4j. Pytanie użytkownka jest tłumaczone przez gpt na
        instrukcje języka Cypher służące do komunikacji z neo4j, następnie mechanizm
        chatbota wykorzystuje te instrukcje i pobiera informacje z bazy neoj4, wyniki
        są przesyłane ponownie do gpt, by ten utworzył na ich podstawie odpowiedź 
        w języku naturalnym, dopiero ta jest prezentowana użytkownikowi.
        </p>

        <p>
        Ciekawy artykuł tego samego autora (Tomaz Bratanic) - "Knowledge Graphs & LLMs:
        Fine-Tuning Vs. Retrieval-Augmented Generation"
        (<a href="https://medium.com/neo4j/knowledge-graphs-llms-fine-tuning-vs-retrieval-augmented-generation-30e875d63a35" target="_blank" rel="noopener">Link</a>)
        omawia oba podejścia do rozszerzania wiedzy modelu - fine-tuning i rozszerzanie
        z użyciem zewnętrznych informacji (retrieval-augmented), zalety i wady obu
        rozwiązań, oraz niektóre narzędzia które są wykorzystywane do pobierania danych
        z zewnętrzych źródeł i przekazywania do kontesktu zapytania dla modelu
        (LangChain, LlamaIndex).
        </p>

        <p>
        Warto zauważyć, że korzystanie z zewnętrznego źródła danych podczas pracy
        z modelem GPT nie musi ograniczać się do jednego źródła, jednego grafu wiedzy,
        można wyobrazić sobie korzystanie równolegle z wielu baz wiedzy i zindekswanych
        korpusów tekstów, krytyczną rolę pełnią tu mechanizmy wyszukiwania w tekstach
        i grafach, od ich jakości zależy jakość odpowiedzi. Ograniczeniem jest oczywiście
        wielkość kontekstu przetwarzanego przez model, można jednak spodziewać się,
        patrząc na prowadzone w tym kierunku prace, że wielkość kontekstu będzie się
        w nowych modelach systematycznie zwiększać.
        </p>

        <p>
        Opublikowane niedawno nowe wersje modeli GPT obsługują nową możliwość , którą
        również da się wykorzystać do połączenia modelu z informacjami np. z grafów wiedzy
        <label for="14-2" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="14-2" class="margin-toggle"/>
        <span class="sidenote">
        <a href="https://openai.com/blog/function-calling-and-other-api-updates" target="_blank" rel="noopener">
        Function calling and other API updates</a>.
        </span>
        .
        Tzw. function calling pozwala zdefiniować funkcje przekazywane w zapytaniu do modelu,
        z których ten może skorzystać jeżeli taka opcja zostanie uznana za korzystną w wyniku
        analizy zapytania. Model zwraca wówczas wynik z nazwą funkcji i sugerowanymi na
        podstawie zapytania parametrami. Na przykład nazwiskiem postaci historycznej której dotyczy
        zapytanie. System korzystający z GPT może wówczas uruchomić daną funkcję pobierając dane
        np. z wikidata.org dotyczące tej postaci i przekazać je w kolejnej iteracji zapytania
        do modelu, który na ich podstawie sformułuje odpowiedź w języku naturalnym. To kolejna
        funkjconalność rozszerzająca możliwości modeli GPT.
        </p>

        <p>
        Patrząc z zewnątrz, spoza świata specjalistów od wielu lat zajmujących się
        grafami wiedzy, można jeszcze zadać sobie jedno (naiwne) pytanie. Swiat KG
        i LLM może oczywiście współistnieć i współpracować. Ale czy wobec gwałtownego
        rozwoju modeli językowych, dostępności szybkich baz wektorowych pracochłonne
        tworzenie grafów wiedzy ma sens? Czy model przetwarzający teksty i wyszukujący
        w nich potrzebną wiedzę, a następnie udzielający odpowiedzi w jezyku naturalnym
        nie jest bardziej oczekiwanym przez użytkowików narzędziem, niż skomplikowane grafy
        wiedzy z ich zawiłymi językami zapytań? Nawet jeżeli ten sam model mógłby skorzystać 
        z danych zawartych w grafie, to jeżeli mechanizmy korzystające z samych
        korpusów tekstów będą szybkie i skuteczne, czy praca włożona w tworzenie
        ustrukturyzowanej wiedzy zwróci się w postaci np. lepszych wyników?
        </p>

        </section>
    </article>

{{end}}
